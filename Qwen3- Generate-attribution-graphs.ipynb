{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1154b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/Attribution-Graph-Qwen-/circuit-tracer\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops>=0.8.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.26.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.34.4)\n",
      "Requirement already satisfied: ipykernel>=6.29.5 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (6.30.1)\n",
      "Requirement already satisfied: ipywidgets>=8.1.7 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (2.11.7)\n",
      "Requirement already satisfied: safetensors>=0.5.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: tokenizers>=0.21.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.22.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: transformer-lens>=v2.16.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (2.16.1)\n",
      "Requirement already satisfied: transformers>=4.50.0 in /opt/miniforge3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (4.56.1)\n",
      "Requirement already satisfied: filelock in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (1.1.10)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (9.5.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/miniforge3/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (5.14.3)\n",
      "Requirement already satisfied: decorator in /opt/miniforge3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/miniforge3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/miniforge3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/miniforge3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/miniforge3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/miniforge3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /opt/miniforge3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/miniforge3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.2.13)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/miniforge3/lib/python3.12/site-packages (from ipywidgets>=8.1.7->circuit-tracer==0.1.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/miniforge3/lib/python3.12/site-packages (from ipywidgets>=8.1.7->circuit-tracer==0.1.0) (3.0.15)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/miniforge3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniforge3/lib/python3.12/site-packages (from jupyter-client>=8.0.0->ipykernel>=6.29.5->circuit-tracer==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/miniforge3/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.29.5->circuit-tracer==0.1.0) (4.4.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/miniforge3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniforge3/lib/python3.12/site-packages (from pydantic>=2.0.0->circuit-tracer==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniforge3/lib/python3.12/site-packages (from pydantic>=2.0.0->circuit-tracer==0.1.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniforge3/lib/python3.12/site-packages (from pydantic>=2.0.0->circuit-tracer==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniforge3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (80.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->circuit-tracer==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.10.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.3.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2.3.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.2.1)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.0.5)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.4.4)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /opt/miniforge3/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.21.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniforge3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniforge3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/miniforge3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/miniforge3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/miniforge3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/miniforge3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.10)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /opt/miniforge3/lib/python3.12/site-packages (from jaxtyping>=0.2.11->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniforge3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniforge3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniforge3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniforge3/lib/python3.12/site-packages (from rich>=12.6.0->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/miniforge3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniforge3/lib/python3.12/site-packages (from transformers>=4.50.0->circuit-tracer==0.1.0) (2025.9.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/miniforge3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/miniforge3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.1.45)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/miniforge3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (6.32.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/miniforge3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2.37.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/miniforge3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/miniforge3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniforge3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->circuit-tracer==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/miniforge3/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/miniforge3/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/miniforge3/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.2.3)\n",
      "Building wheels for collected packages: circuit-tracer\n",
      "  Building editable for circuit-tracer (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuit-tracer: filename=circuit_tracer-0.1.0-py3-none-any.whl size=6276 sha256=2c87e84c24f3f689918577f7bf89205ab3d12e70b6530587d82f51e82eaf2a86\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mrl1k3ua/wheels/76/ab/fc/ab83556a0dc31920cedf3577d2da40580dd6f44d0b6ec2989a\n",
      "Successfully built circuit-tracer\n",
      "Installing collected packages: circuit-tracer\n",
      "  Attempting uninstall: circuit-tracer\n",
      "    Found existing installation: circuit-tracer 0.1.0\n",
      "    Uninstalling circuit-tracer-0.1.0:\n",
      "      Successfully uninstalled circuit-tracer-0.1.0\n",
      "Successfully installed circuit-tracer-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e circuit-tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733bebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this in a cell before importing torch/transformers\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf6664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab Setup Environment\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    !mkdir -p repository && cd repository && \\\n",
    "     git clone https://github.com/safety-research/circuit-tracer && \\\n",
    "     curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n",
    "     uv pip install -e circuit-tracer/\n",
    "\n",
    "    import sys\n",
    "    from huggingface_hub import notebook_login\n",
    "    sys.path.append('repository/circuit-tracer')\n",
    "    sys.path.append('repository/circuit-tracer/demos')\n",
    "    notebook_login(new_session=False)\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77faf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "from huggingface_hub import get_token, hf_api, hf_hub_download, snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e33c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from collections.abc import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "from circuit_tracer.attribution.context import AttributionContext\n",
    "from circuit_tracer.transcoder import TranscoderSet\n",
    "from circuit_tracer.transcoder.cross_layer_transcoder import CrossLayerTranscoder\n",
    "from circuit_tracer.utils import get_default_device\n",
    "from circuit_tracer.utils.hf_utils import load_transcoder_from_hub\n",
    "\n",
    "# Type definition for an intervention tuple (layer, position, feature_idx, value)\n",
    "Intervention = tuple[\n",
    "    int | torch.Tensor, int | slice | torch.Tensor, int | torch.Tensor, int | torch.Tensor\n",
    "]\n",
    "\n",
    "\n",
    "class ReplacementMLP(nn.Module):\n",
    "    \"\"\"Wrapper for a TransformerLens MLP layer that adds in extra hooks\"\"\"\n",
    "\n",
    "    def __init__(self, old_mlp: nn.Module):\n",
    "        super().__init__()\n",
    "        self.old_mlp = old_mlp\n",
    "        self.hook_in = HookPoint()\n",
    "        self.hook_out = HookPoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hook_in(x)\n",
    "        mlp_out = self.old_mlp(x)\n",
    "        return self.hook_out(mlp_out)\n",
    "\n",
    "\n",
    "class ReplacementUnembed(nn.Module):\n",
    "    \"\"\"Wrapper for a TransformerLens Unembed layer that adds in extra hooks\"\"\"\n",
    "\n",
    "    def __init__(self, old_unembed: nn.Module):\n",
    "        super().__init__()\n",
    "        self.old_unembed = old_unembed\n",
    "        self.hook_pre = HookPoint()\n",
    "        self.hook_post = HookPoint()\n",
    "\n",
    "    @property\n",
    "    def W_U(self):\n",
    "        return self.old_unembed.W_U\n",
    "\n",
    "    @property\n",
    "    def b_U(self):\n",
    "        return self.old_unembed.b_U\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hook_pre(x)\n",
    "        x = self.old_unembed(x)\n",
    "        return self.hook_post(x)\n",
    "\n",
    "\n",
    "class ReplacementModel(HookedTransformer):\n",
    "    transcoders: TranscoderSet | CrossLayerTranscoder  # Support both types\n",
    "    feature_input_hook: str\n",
    "    feature_output_hook: str\n",
    "    skip_transcoder: bool\n",
    "    scan: str | list[str] | None\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls,\n",
    "        config: HookedTransformerConfig,\n",
    "        transcoders: TranscoderSet | CrossLayerTranscoder,  # Accept both\n",
    "        **kwargs,\n",
    "    ) -> \"ReplacementModel\":\n",
    "        \"\"\"Create a ReplacementModel from a given HookedTransformerConfig and TranscoderSet\n",
    "\n",
    "        Args:\n",
    "            config (HookedTransformerConfig): the config of the HookedTransformer\n",
    "            transcoders (TranscoderSet): The transcoder set with configuration\n",
    "\n",
    "        Returns:\n",
    "            ReplacementModel: The loaded ReplacementModel\n",
    "        \"\"\"\n",
    "        model = cls(config, **kwargs)\n",
    "        model._configure_replacement_model(transcoders)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained_and_transcoders(\n",
    "        cls,\n",
    "        model_name: str,\n",
    "        transcoders: TranscoderSet | CrossLayerTranscoder,  # Accept both\n",
    "        **kwargs,\n",
    "    ) -> \"ReplacementModel\":\n",
    "        \"\"\"Create a ReplacementModel from the name of HookedTransformer and TranscoderSet\n",
    "\n",
    "        Args:\n",
    "            model_name (str): the name of the pretrained HookedTransformer\n",
    "            transcoders (TranscoderSet): The transcoder set with configuration\n",
    "\n",
    "        Returns:\n",
    "            ReplacementModel: The loaded ReplacementModel\n",
    "        \"\"\"\n",
    "        model = super().from_pretrained(\n",
    "            model_name,\n",
    "            fold_ln=False,\n",
    "            center_writing_weights=False,\n",
    "            center_unembed=False,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        model._configure_replacement_model(transcoders)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name: str,\n",
    "        transcoder_set: str,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        **kwargs,\n",
    "    ) -> \"ReplacementModel\":\n",
    "        \"\"\"Create a ReplacementModel from model name and transcoder config\n",
    "\n",
    "        Args:\n",
    "            model_name (str): the name of the pretrained HookedTransformer\n",
    "            transcoder_set (str): Either a predefined transcoder set name, or a config file\n",
    "\n",
    "        Returns:\n",
    "            ReplacementModel: The loaded ReplacementModel\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = get_default_device()\n",
    "\n",
    "        transcoders, _ = load_transcoder_from_hub(transcoder_set, device=device, dtype=dtype)\n",
    "\n",
    "        return cls.from_pretrained_and_transcoders(\n",
    "            model_name,\n",
    "            transcoders,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            **kwargs,\n",
    "        )\n",
    "    # ---- Public constructors (keep API parity) ----\n",
    "    def ablate_attention_heads(self, heads_to_ablate):\n",
    "        \"\"\"\n",
    "        Register forward hooks to ablate (zero out) specific attention heads.\n",
    "        heads_to_ablate: list of (layer_idx, head_idx) tuples\n",
    "        \"\"\"\n",
    "        self._ablation_hooks = getattr(self, \"_ablation_hooks\", [])\n",
    "        # Remove any previous ablation hooks\n",
    "        for h in self._ablation_hooks:\n",
    "            h.remove()\n",
    "        self._ablation_hooks = []\n",
    "\n",
    "        def make_ablation_hook(head_idx):\n",
    "            def ablation_hook(module, input, output):\n",
    "                # output: (batch, seq, hidden)\n",
    "                attn_out = output[0] if isinstance(output, tuple) else output\n",
    "                num_heads = getattr(module, \"num_heads\", getattr(module, \"num_attention_heads\", None))\n",
    "                if num_heads is None:\n",
    "                    raise AttributeError(\"Could not determine number of attention heads\")\n",
    "                head_dim = attn_out.shape[-1] // num_heads\n",
    "                attn_out = attn_out.view(attn_out.shape[0], attn_out.shape[1], num_heads, head_dim)\n",
    "                attn_out[:, :, head_idx, :] = 0\n",
    "                attn_out = attn_out.view(attn_out.shape[0], attn_out.shape[1], -1)\n",
    "                if isinstance(output, tuple):\n",
    "                    return (attn_out,) + output[1:]\n",
    "                else:\n",
    "                    return attn_out\n",
    "            return ablation_hook\n",
    "\n",
    "        for layer_idx, head_idx in heads_to_ablate:\n",
    "            # Try to find the attention module\n",
    "            try:\n",
    "                attn_mod = self._get_layer_module(layer_idx, \"self_attn\")\n",
    "            except AttributeError:\n",
    "                attn_mod = self._get_layer_module(layer_idx, \"attention\")\n",
    "            hook = attn_mod.register_forward_hook(make_ablation_hook(head_idx))\n",
    "            self._ablation_hooks.append(hook)\n",
    "\n",
    "    def clear_ablation_hooks(self):\n",
    "        \"\"\"Remove all ablation hooks.\"\"\"\n",
    "        if hasattr(self, \"_ablation_hooks\"):\n",
    "            for h in self._ablation_hooks:\n",
    "                h.remove()\n",
    "            self._ablation_hooks = []\n",
    "            \n",
    "    def _configure_replacement_model(self, transcoder_set: TranscoderSet | CrossLayerTranscoder):\n",
    "        transcoder_set.to(self.cfg.device, self.cfg.dtype)\n",
    "\n",
    "        self.transcoders = transcoder_set\n",
    "        self.feature_input_hook = transcoder_set.feature_input_hook\n",
    "        self.original_feature_output_hook = transcoder_set.feature_output_hook\n",
    "        self.feature_output_hook = transcoder_set.feature_output_hook + \".hook_out_grad\"\n",
    "        self.skip_transcoder = transcoder_set.skip_connection\n",
    "        self.scan = transcoder_set.scan\n",
    "\n",
    "        for block in self.blocks:\n",
    "            block.mlp = ReplacementMLP(block.mlp)  # type: ignore\n",
    "\n",
    "        self.unembed = ReplacementUnembed(self.unembed)\n",
    "\n",
    "        self._configure_gradient_flow()\n",
    "        self._deduplicate_attention_buffers()\n",
    "        self.setup()\n",
    "\n",
    "    def _configure_gradient_flow(self):\n",
    "        if isinstance(self.transcoders, TranscoderSet):\n",
    "            for layer, transcoder in enumerate(self.transcoders):\n",
    "                self._configure_skip_connection(self.blocks[layer], transcoder)\n",
    "        else:\n",
    "            for layer in range(self.cfg.n_layers):\n",
    "                self._configure_skip_connection(self.blocks[layer], self.transcoders)\n",
    "\n",
    "        def stop_gradient(acts, hook):\n",
    "            return acts.detach()\n",
    "\n",
    "        for block in self.blocks:\n",
    "            block.attn.hook_pattern.add_hook(stop_gradient, is_permanent=True)  # type: ignore\n",
    "            block.ln1.hook_scale.add_hook(stop_gradient, is_permanent=True)  # type: ignore\n",
    "            block.ln2.hook_scale.add_hook(stop_gradient, is_permanent=True)  # type: ignore\n",
    "            if hasattr(block, \"ln1_post\"):\n",
    "                block.ln1_post.hook_scale.add_hook(stop_gradient, is_permanent=True)  # type: ignore\n",
    "            if hasattr(block, \"ln2_post\"):\n",
    "                block.ln2_post.hook_scale.add_hook(stop_gradient, is_permanent=True)  # type: ignore\n",
    "            self.ln_final.hook_scale.add_hook(stop_gradient, is_permanent=True)  # type: ignore\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        def enable_gradient(tensor, hook):\n",
    "            tensor.requires_grad = True\n",
    "            return tensor\n",
    "\n",
    "        self.hook_embed.add_hook(enable_gradient, is_permanent=True)\n",
    "\n",
    "    def _configure_skip_connection(self, block, transcoder):\n",
    "        cached = {}\n",
    "\n",
    "        def cache_activations(acts, hook):\n",
    "            cached[\"acts\"] = acts\n",
    "\n",
    "        def add_skip_connection(acts: torch.Tensor, hook: HookPoint, grad_hook: HookPoint):\n",
    "            # We add grad_hook because we need a way to hook into the gradients of the output\n",
    "            # of this function. If we put the backwards hook here at hook, the grads will be 0\n",
    "            # because we detached acts.\n",
    "            skip_input_activation = cached.pop(\"acts\")\n",
    "            if hasattr(transcoder, \"W_skip\") and transcoder.W_skip is not None:\n",
    "                skip = transcoder.compute_skip(skip_input_activation)\n",
    "            else:\n",
    "                skip = skip_input_activation * 0\n",
    "            return grad_hook(skip + (acts - skip).detach())\n",
    "\n",
    "        # add feature input hook\n",
    "        output_hook_parts = self.feature_input_hook.split(\".\")\n",
    "        subblock = block\n",
    "        for part in output_hook_parts:\n",
    "            subblock = getattr(subblock, part)\n",
    "        subblock.add_hook(cache_activations, is_permanent=True)\n",
    "\n",
    "        # add feature output hook and special grad hook\n",
    "        output_hook_parts = self.original_feature_output_hook.split(\".\")\n",
    "        subblock = block\n",
    "        for part in output_hook_parts:\n",
    "            subblock = getattr(subblock, part)\n",
    "        subblock.hook_out_grad = HookPoint()\n",
    "        subblock.add_hook(\n",
    "            partial(add_skip_connection, grad_hook=subblock.hook_out_grad),\n",
    "            is_permanent=True,\n",
    "        )\n",
    "\n",
    "    def _deduplicate_attention_buffers(self):\n",
    "        \"\"\"\n",
    "        Share attention buffers across layers to save memory.\n",
    "\n",
    "        TransformerLens makes separate copies of the same masks and RoPE\n",
    "        embeddings for each layer - This just keeps one copy\n",
    "        of each and shares it across all layers.\n",
    "        \"\"\"\n",
    "\n",
    "        attn_masks = {}\n",
    "\n",
    "        for block in self.blocks:\n",
    "            attn_masks[block.attn.attn_type] = block.attn.mask  # type: ignore\n",
    "            if hasattr(block.attn, \"rotary_sin\"):\n",
    "                attn_masks[\"rotary_sin\"] = block.attn.rotary_sin  # type: ignore\n",
    "                attn_masks[\"rotary_cos\"] = block.attn.rotary_cos  # type: ignore\n",
    "\n",
    "        for block in self.blocks:\n",
    "            block.attn.mask = attn_masks[block.attn.attn_type]  # type: ignore\n",
    "            if hasattr(block.attn, \"rotary_sin\"):\n",
    "                block.attn.rotary_sin = attn_masks[\"rotary_sin\"]  # type: ignore\n",
    "                block.attn.rotary_cos = attn_masks[\"rotary_cos\"]  # type: ignore\n",
    "\n",
    "    def _get_activation_caching_hooks(\n",
    "        self,\n",
    "        sparse: bool = False,\n",
    "        apply_activation_function: bool = True,\n",
    "        append: bool = False,\n",
    "    ) -> tuple[list[torch.Tensor], list[tuple[str, Callable]]]:\n",
    "        activation_matrix = (\n",
    "            [[] for _ in range(self.cfg.n_layers)] if append else [None] * self.cfg.n_layers\n",
    "        )\n",
    "\n",
    "        def cache_activations(acts, hook, layer):\n",
    "            transcoder_acts = (\n",
    "                self.transcoders.encode_layer(\n",
    "                    acts, layer, apply_activation_function=apply_activation_function\n",
    "                )\n",
    "                .detach()\n",
    "                .squeeze(0)\n",
    "            )\n",
    "            if sparse:\n",
    "                transcoder_acts = transcoder_acts.to_sparse()\n",
    "\n",
    "            if append:\n",
    "                activation_matrix[layer].append(transcoder_acts)\n",
    "            else:\n",
    "                activation_matrix[layer] = transcoder_acts  # type: ignore\n",
    "\n",
    "        activation_hooks = [\n",
    "            (\n",
    "                f\"blocks.{layer}.{self.feature_input_hook}\",\n",
    "                partial(cache_activations, layer=layer),\n",
    "            )\n",
    "            for layer in range(self.cfg.n_layers)\n",
    "        ]\n",
    "        return activation_matrix, activation_hooks  # type: ignore\n",
    "\n",
    "    def get_activations(\n",
    "        self,\n",
    "        inputs: str | torch.Tensor,\n",
    "        sparse: bool = False,\n",
    "        apply_activation_function: bool = True,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get the transcoder activations for a given prompt\n",
    "\n",
    "        Args:\n",
    "            inputs (str | torch.Tensor): The inputs you want to get activations over\n",
    "            sparse (bool, optional): Whether to return a sparse tensor of activations.\n",
    "                Useful if d_transcoder is large. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: the model logits on the inputs and the\n",
    "                associated activation cache\n",
    "        \"\"\"\n",
    "\n",
    "        activation_cache, activation_hooks = self._get_activation_caching_hooks(\n",
    "            sparse=sparse,\n",
    "            apply_activation_function=apply_activation_function,\n",
    "        )\n",
    "        with torch.inference_mode(), self.hooks(activation_hooks):  # type: ignore\n",
    "            logits = self(inputs)\n",
    "        activation_cache = torch.stack(activation_cache)\n",
    "        if sparse:\n",
    "            activation_cache = activation_cache.coalesce()\n",
    "        return logits, activation_cache\n",
    "\n",
    "    @contextmanager\n",
    "    def zero_softcap(self):\n",
    "        current_softcap = self.cfg.output_logits_soft_cap\n",
    "        try:\n",
    "            self.cfg.output_logits_soft_cap = 0.0\n",
    "            yield\n",
    "        finally:\n",
    "            self.cfg.output_logits_soft_cap = current_softcap\n",
    "\n",
    "    def ensure_tokenized(self, prompt: str | torch.Tensor | list[int]) -> torch.Tensor:\n",
    "        \"\"\"Convert prompt to 1-D tensor of token ids with proper special token handling.\n",
    "\n",
    "        This method ensures that a special token (BOS/PAD) is prepended to the input sequence.\n",
    "        The first token position in transformer models typically exhibits unusually high norm\n",
    "        and an excessive number of active features due to how models process the beginning of\n",
    "        sequences. By prepending a special token, we ensure that actual content tokens have\n",
    "        more consistent and interpretable feature activations, avoiding the artifacts present\n",
    "        at position 0. This prepended token is later ignored during attribution analysis.\n",
    "\n",
    "        Args:\n",
    "            prompt: String, tensor, or list of token ids representing a single sequence\n",
    "\n",
    "        Returns:\n",
    "            1-D tensor of token ids with BOS/PAD token at the beginning\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If prompt is not str, tensor, or list\n",
    "            ValueError: If tensor has wrong shape (must be 1-D or 2-D with batch size 1)\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            tokens = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        elif isinstance(prompt, torch.Tensor):\n",
    "            tokens = prompt.squeeze()\n",
    "        elif isinstance(prompt, list):\n",
    "            tokens = torch.tensor(prompt, dtype=torch.long).squeeze()\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported prompt type: {type(prompt)}\")\n",
    "\n",
    "        if tokens.ndim > 1:\n",
    "            raise ValueError(f\"Tensor must be 1-D, got shape {tokens.shape}\")\n",
    "\n",
    "        # Check if a special token is already present at the beginning\n",
    "        if tokens[0] in self.tokenizer.all_special_ids:\n",
    "            return tokens.to(self.cfg.device)\n",
    "\n",
    "        # Prepend a special token to avoid artifacts at position 0\n",
    "        candidate_bos_token_ids = [\n",
    "            self.tokenizer.bos_token_id,\n",
    "            self.tokenizer.pad_token_id,\n",
    "            self.tokenizer.eos_token_id,\n",
    "        ]\n",
    "        candidate_bos_token_ids += self.tokenizer.all_special_ids\n",
    "\n",
    "        dummy_bos_token_id = next(filter(None, candidate_bos_token_ids))\n",
    "        if dummy_bos_token_id is None:\n",
    "            warnings.warn(\n",
    "                \"No suitable special token found for BOS token replacement. \"\n",
    "                \"The first token will be ignored.\"\n",
    "            )\n",
    "        else:\n",
    "            tokens = torch.cat([torch.tensor([dummy_bos_token_id], device=tokens.device), tokens])\n",
    "\n",
    "        return tokens.to(self.cfg.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def setup_attribution(self, inputs: str | torch.Tensor):\n",
    "        \"\"\"Precomputes the transcoder activations and error vectors, saving them and the\n",
    "        token embeddings.\n",
    "\n",
    "        Args:\n",
    "            inputs (str): the inputs to attribute - hard coded to be a single string (no\n",
    "                batching) for now\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(inputs, str):\n",
    "            tokens = self.ensure_tokenized(inputs)\n",
    "        else:\n",
    "            tokens = inputs.squeeze()\n",
    "\n",
    "        assert isinstance(tokens, torch.Tensor), \"Tokens must be a tensor\"\n",
    "        assert tokens.ndim == 1, \"Tokens must be a 1D tensor\"\n",
    "\n",
    "        mlp_in_cache, mlp_in_caching_hooks, _ = self.get_caching_hooks(\n",
    "            lambda name: self.feature_input_hook in name\n",
    "        )\n",
    "\n",
    "        mlp_out_cache, mlp_out_caching_hooks, _ = self.get_caching_hooks(\n",
    "            lambda name: self.feature_output_hook in name\n",
    "        )\n",
    "        logits = self.run_with_hooks(tokens, fwd_hooks=mlp_in_caching_hooks + mlp_out_caching_hooks)\n",
    "\n",
    "        mlp_in_cache = torch.cat(list(mlp_in_cache.values()), dim=0)\n",
    "        mlp_out_cache = torch.cat(list(mlp_out_cache.values()), dim=0)\n",
    "\n",
    "        attribution_data = self.transcoders.compute_attribution_components(mlp_in_cache)\n",
    "\n",
    "        # Compute error vectors\n",
    "        error_vectors = mlp_out_cache - attribution_data[\"reconstruction\"]\n",
    "\n",
    "        error_vectors[:, 0] = 0\n",
    "        token_vectors = self.W_E[tokens].detach()  # (n_pos, d_model)\n",
    "\n",
    "        return AttributionContext(\n",
    "            activation_matrix=attribution_data[\"activation_matrix\"],\n",
    "            logits=logits,\n",
    "            error_vectors=error_vectors,\n",
    "            token_vectors=token_vectors,\n",
    "            decoder_vecs=attribution_data[\"decoder_vecs\"],\n",
    "            encoder_vecs=attribution_data[\"encoder_vecs\"],\n",
    "            encoder_to_decoder_map=attribution_data[\"encoder_to_decoder_map\"],\n",
    "            decoder_locations=attribution_data[\"decoder_locations\"],\n",
    "        )\n",
    "\n",
    "    def setup_intervention_with_freeze(\n",
    "        self, inputs: str | torch.Tensor, constrained_layers: range | None = None\n",
    "    ) -> tuple[torch.Tensor, list[tuple[str, Callable]]]:\n",
    "        \"\"\"Sets up an intervention with either frozen attention + LayerNorm(default) or frozen\n",
    "        attention, LayerNorm, and MLPs, for constrained layers\n",
    "\n",
    "        Args:\n",
    "            inputs (Union[str, torch.Tensor]): The inputs to intervene on\n",
    "            constrained_layers (range | None): whether to apply interventions only to a certain\n",
    "                range. Mostly applicable to CLTs. If the given range includes all model layers,\n",
    "                we also freeze layernorm denominators, computing direct effects. None means no\n",
    "                constraints (iterative patching)\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[str, Callable]]: The freeze hooks needed to run the desired intervention.\n",
    "        \"\"\"\n",
    "\n",
    "        hookpoints_to_freeze = [\"hook_pattern\"]\n",
    "        if constrained_layers:\n",
    "            if set(range(self.cfg.n_layers)).issubset(set(constrained_layers)):\n",
    "                hookpoints_to_freeze.append(\"hook_scale\")\n",
    "            hookpoints_to_freeze.append(self.feature_output_hook)\n",
    "            if self.skip_transcoder:\n",
    "                hookpoints_to_freeze.append(self.feature_input_hook)\n",
    "\n",
    "        # only freeze outputs in constrained range\n",
    "        selected_hook_points = []\n",
    "        for hook_point, hook_obj in self.hook_dict.items():\n",
    "            if any(\n",
    "                hookpoint_to_freeze in hook_point for hookpoint_to_freeze in hookpoints_to_freeze\n",
    "            ):\n",
    "                # don't freeze feature outputs if the layer is not in the constrained range\n",
    "                if (\n",
    "                    self.feature_output_hook in hook_point\n",
    "                    and constrained_layers\n",
    "                    and hook_obj.layer() not in constrained_layers\n",
    "                ):\n",
    "                    continue\n",
    "                selected_hook_points.append(hook_point)\n",
    "\n",
    "        freeze_cache, cache_hooks, _ = self.get_caching_hooks(names_filter=selected_hook_points)\n",
    "\n",
    "        original_activations, activation_caching_hooks = self._get_activation_caching_hooks()\n",
    "        self.run_with_hooks(inputs, fwd_hooks=cache_hooks + activation_caching_hooks)\n",
    "\n",
    "        def freeze_hook(activations, hook):\n",
    "            cached_values = freeze_cache[hook.name]\n",
    "\n",
    "            assert activations.shape == cached_values.shape, (\n",
    "                f\"Activations shape {activations.shape} does not match cached values\"\n",
    "                f\" shape {cached_values.shape} at hook {hook.name}\"\n",
    "            )\n",
    "            return cached_values\n",
    "\n",
    "        fwd_hooks = [\n",
    "            (hookpoint, freeze_hook)\n",
    "            for hookpoint in freeze_cache.keys()\n",
    "            if self.feature_input_hook not in hookpoint\n",
    "        ]\n",
    "\n",
    "        if not (constrained_layers and self.skip_transcoder):\n",
    "            return torch.stack(original_activations), fwd_hooks\n",
    "\n",
    "        skip_diffs = {}\n",
    "\n",
    "        def diff_hook(activations, hook, layer: int):\n",
    "            # The MLP hook out freeze hook sets the value of the MLP to the value it\n",
    "            # had when run on the inputs normally. We subtract out the skip that\n",
    "            # corresponds to such a run, and add in the skip with direct effects.\n",
    "            assert not isinstance(self.transcoders, CrossLayerTranscoder), \"Skip CLTs forbidden\"\n",
    "            frozen_skip = self.transcoders[layer].compute_skip(freeze_cache[hook.name])\n",
    "            normal_skip = self.transcoders[layer].compute_skip(activations)\n",
    "\n",
    "            skip_diffs[layer] = normal_skip - frozen_skip\n",
    "\n",
    "        def add_diff_hook(activations, hook, layer: int):\n",
    "            # open-ended generation case\n",
    "            return activations + skip_diffs[layer]\n",
    "\n",
    "        fwd_hooks += [\n",
    "            (f\"blocks.{layer}.{self.feature_input_hook}\", partial(diff_hook, layer=layer))\n",
    "            for layer in constrained_layers\n",
    "        ]\n",
    "        fwd_hooks += [\n",
    "            (f\"blocks.{layer}.{self.feature_output_hook}\", partial(add_diff_hook, layer=layer))\n",
    "            for layer in constrained_layers\n",
    "        ]\n",
    "        return torch.stack(original_activations), fwd_hooks\n",
    "\n",
    "    def _get_feature_intervention_hooks(\n",
    "        self,\n",
    "        inputs: str | torch.Tensor,\n",
    "        interventions: list[Intervention],\n",
    "        constrained_layers: range | None = None,\n",
    "        freeze_attention: bool = True,\n",
    "        apply_activation_function: bool = True,\n",
    "        sparse: bool = False,\n",
    "        using_past_kv_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"Given the input, and a dictionary of features to intervene on, performs the\n",
    "        intervention, allowing all effects to propagate (optionally allowing its effects to\n",
    "        propagate through transcoders)\n",
    "\n",
    "        Args:\n",
    "            input (_type_): the input prompt to intervene on\n",
    "            intervention_dict (List[Intervention]): A list of interventions to perform, formatted\n",
    "                as a list of (layer, position, feature_idx, value)\n",
    "            constrained_layers (range | None): whether to apply interventions only to a certain\n",
    "                range, freezing all MLPs within the layer range before doing so. This is mostly\n",
    "                applicable to CLTs. If the given range includes all model layers, we also freeze\n",
    "                layernorm denominators, computing direct effects.nNone means no constraints\n",
    "                (iterative patching)\n",
    "            apply_activation_function (bool): whether to apply the activation function when\n",
    "                recording the activations to be returned. This is useful to set to False for\n",
    "                testing purposes, as attribution predicts the change in pre-activation\n",
    "                feature values.\n",
    "            sparse (bool): whether to sparsify the activations in the returned cache. Setting\n",
    "                this to True will take up less memory, at the expense of slower interventions.\n",
    "            using_past_kv_cache (bool): whether we are generating with past_kv_cache, meaning that\n",
    "                n_pos is 1, and we must append onto the existing logit / activation cache if the\n",
    "                hooks are run multiple times. Defaults to False\n",
    "        \"\"\"\n",
    "\n",
    "        interventions_by_layer = defaultdict(list)\n",
    "        for layer, pos, feature_idx, value in interventions:\n",
    "            interventions_by_layer[layer].append((pos, feature_idx, value))\n",
    "\n",
    "        if using_past_kv_cache:\n",
    "            # We're generating one token at a time\n",
    "            original_activations, freeze_hooks = [], []\n",
    "            n_pos = 1\n",
    "        elif (freeze_attention or constrained_layers) and interventions:\n",
    "            original_activations, freeze_hooks = self.setup_intervention_with_freeze(\n",
    "                inputs, constrained_layers=constrained_layers\n",
    "            )\n",
    "            n_pos = original_activations.size(1)\n",
    "        else:\n",
    "            original_activations, freeze_hooks = [], []\n",
    "            if isinstance(inputs, torch.Tensor):\n",
    "                n_pos = inputs.size(0)\n",
    "            else:\n",
    "                n_pos = len(self.tokenizer(inputs).input_ids)\n",
    "\n",
    "        layer_deltas = torch.zeros(\n",
    "            [self.cfg.n_layers, n_pos, self.cfg.d_model],\n",
    "            dtype=self.cfg.dtype,\n",
    "            device=self.cfg.device,\n",
    "        )\n",
    "\n",
    "        # This activation cache will fill up during our forward intervention pass\n",
    "        activation_cache, activation_hooks = self._get_activation_caching_hooks(\n",
    "            apply_activation_function=apply_activation_function,\n",
    "            sparse=sparse,\n",
    "            append=using_past_kv_cache,\n",
    "        )\n",
    "\n",
    "        def calculate_delta_hook(activations, hook, layer: int, layer_interventions):\n",
    "            if constrained_layers:\n",
    "                # base deltas on original activations; don't let effects propagate\n",
    "                transcoder_activations = original_activations[layer]\n",
    "            else:\n",
    "                # recompute deltas based on current activations\n",
    "                transcoder_activations = (\n",
    "                    activation_cache[layer][-1] if using_past_kv_cache else activation_cache[layer]\n",
    "                )\n",
    "                if transcoder_activations.is_sparse:\n",
    "                    transcoder_activations = transcoder_activations.to_dense()\n",
    "\n",
    "                if not apply_activation_function:\n",
    "                    transcoder_activations = self.transcoders.apply_activation_function(\n",
    "                        layer, transcoder_activations.unsqueeze(0)\n",
    "                    ).squeeze(0)\n",
    "\n",
    "            activation_deltas = torch.zeros_like(transcoder_activations)\n",
    "            for pos, feature_idx, value in layer_interventions:\n",
    "                activation_deltas[pos, feature_idx] = (\n",
    "                    value - transcoder_activations[pos, feature_idx]\n",
    "                )\n",
    "\n",
    "            poss, feature_idxs = activation_deltas.nonzero(as_tuple=True)\n",
    "            new_values = activation_deltas[poss, feature_idxs]\n",
    "\n",
    "            decoder_vectors = self.transcoders._get_decoder_vectors(layer, feature_idxs)\n",
    "\n",
    "            if decoder_vectors.ndim == 2:\n",
    "                # Single-layer transcoder case: [n_feature_idxs, d_model]\n",
    "                decoder_vectors = decoder_vectors * new_values.unsqueeze(1)\n",
    "                layer_deltas[layer].index_add_(0, poss, decoder_vectors)\n",
    "            else:\n",
    "                # Cross-layer transcoder case: [n_feature_idxs, n_remaining_layers, d_model]\n",
    "                decoder_vectors = decoder_vectors * new_values.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "                # Transpose to [n_remaining_layers, n_feature_idxs, d_model]\n",
    "                decoder_vectors = decoder_vectors.transpose(0, 1)\n",
    "\n",
    "                # Distribute decoder vectors across layers\n",
    "                n_remaining_layers = decoder_vectors.shape[0]\n",
    "                layer_deltas[-n_remaining_layers:].index_add_(1, poss, decoder_vectors)\n",
    "\n",
    "        def intervention_hook(activations, hook, layer: int):\n",
    "            new_acts = activations\n",
    "            if layer in intervention_range:\n",
    "                new_acts = new_acts + layer_deltas[layer]\n",
    "            layer_deltas[layer] *= 0  # clearing this is important for multi-token generation\n",
    "            return new_acts\n",
    "\n",
    "        delta_hooks = [\n",
    "            (\n",
    "                f\"blocks.{layer}.{self.feature_output_hook}\",\n",
    "                partial(calculate_delta_hook, layer=layer, layer_interventions=layer_interventions),\n",
    "            )\n",
    "            for layer, layer_interventions in interventions_by_layer.items()\n",
    "        ]\n",
    "\n",
    "        intervention_range = constrained_layers if constrained_layers else range(self.cfg.n_layers)\n",
    "        intervention_hooks = [\n",
    "            (f\"blocks.{layer}.{self.feature_output_hook}\", partial(intervention_hook, layer=layer))\n",
    "            for layer in range(self.cfg.n_layers)\n",
    "        ]\n",
    "\n",
    "        all_hooks = freeze_hooks + activation_hooks + delta_hooks + intervention_hooks\n",
    "        cached_logits = [] if using_past_kv_cache else [None]\n",
    "\n",
    "        def logit_cache_hook(activations, hook):\n",
    "            # we need to manually apply the softcap (if used by the model), as it comes post-hook\n",
    "            if self.cfg.output_logits_soft_cap > 0.0:\n",
    "                logits = self.cfg.output_logits_soft_cap * F.tanh(\n",
    "                    activations / self.cfg.output_logits_soft_cap\n",
    "                )\n",
    "            else:\n",
    "                logits = activations.clone()\n",
    "            if using_past_kv_cache:\n",
    "                cached_logits.append(logits)\n",
    "            else:\n",
    "                cached_logits[0] = logits\n",
    "\n",
    "        all_hooks.append((\"unembed.hook_post\", logit_cache_hook))\n",
    "\n",
    "        return all_hooks, cached_logits, activation_cache\n",
    "\n",
    "    @torch.no_grad\n",
    "    def feature_intervention(\n",
    "        self,\n",
    "        inputs: str | torch.Tensor,\n",
    "        interventions: list[Intervention],\n",
    "        constrained_layers: range | None = None,\n",
    "        freeze_attention: bool = True,\n",
    "        apply_activation_function: bool = True,\n",
    "        sparse: bool = False,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Given the input, and a dictionary of features to intervene on, performs the\n",
    "        intervention, and returns the logits and feature activations. If freeze_attention or\n",
    "        constrained_layers is True, attention patterns will be frozen, along with MLPs and\n",
    "        LayerNorms. If constrained_layers is set, the effects of intervention will not propagate\n",
    "        through the constrained layers, and CLTs will write only to those layers. Otherwise, the\n",
    "        effects of the intervention will propagate through transcoders / LayerNorms\n",
    "\n",
    "        Args:\n",
    "            input (_type_): the input prompt to intervene on\n",
    "            interventions (list[tuple[int, Union[int, slice, torch.Tensor]], int,\n",
    "                Union[int, torch.Tensor]]): A list of interventions to perform, formatted as\n",
    "                a list of (layer, position, feature_idx, value)\n",
    "            constrained_layers (range | None): whether to apply interventions only to a certain\n",
    "                range. Mostly applicable to CLTs. If the given range includes all model layers,\n",
    "                we also freeze layernorm denominators, computing direct effects. None means no\n",
    "                constraints (iterative patching)\n",
    "            freeze_attention (bool): whether to freeze all attention patterns an layernorms\n",
    "            apply_activation_function (bool): whether to apply the activation function when\n",
    "                recording the activations to be returned. This is useful to set to False for\n",
    "                testing purposes, as attribution predicts the change in pre-activation\n",
    "                feature values.\n",
    "            sparse (bool): whether to sparsify the activations in the returned cache. Setting\n",
    "                this to True will take up less memory, at the expense of slower interventions.\n",
    "        \"\"\"\n",
    "\n",
    "        hooks, _, activation_cache = self._get_feature_intervention_hooks(\n",
    "            inputs,\n",
    "            interventions,\n",
    "            constrained_layers=constrained_layers,\n",
    "            freeze_attention=freeze_attention,\n",
    "            apply_activation_function=apply_activation_function,\n",
    "            sparse=sparse,\n",
    "        )\n",
    "\n",
    "        with self.hooks(hooks):  # type: ignore\n",
    "            logits = self(inputs)\n",
    "\n",
    "        activation_cache = torch.stack(activation_cache)\n",
    "\n",
    "        return logits, activation_cache\n",
    "\n",
    "    def _convert_open_ended_interventions(\n",
    "        self,\n",
    "        interventions: list[Intervention],\n",
    "    ) -> list[Intervention]:\n",
    "        \"\"\"Convert open-ended interventions into position-0 equivalents.\n",
    "\n",
    "        An intervention is *open-ended* if its position component is a ``slice`` whose\n",
    "        ``stop`` attribute is ``None`` (e.g. ``slice(1, None)``). Such interventions will\n",
    "        also apply to tokens generated in an open-ended generation loop. In such cases,\n",
    "        when use_past_kv_cache=True, the model only runs the most recent token\n",
    "        (and there is thus only 1 position).\n",
    "        \"\"\"\n",
    "        converted = []\n",
    "        for layer, pos, feature_idx, value in interventions:\n",
    "            if isinstance(pos, slice) and pos.stop is None:\n",
    "                converted.append((layer, 0, feature_idx, value))\n",
    "        return converted\n",
    "\n",
    "    @torch.no_grad\n",
    "    def feature_intervention_generate(\n",
    "        self,\n",
    "        inputs: str | torch.Tensor,\n",
    "        interventions: list[Intervention],\n",
    "        constrained_layers: range | None = None,\n",
    "        freeze_attention: bool = True,\n",
    "        apply_activation_function: bool = True,\n",
    "        sparse: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> tuple[str, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Given the input, and a dictionary of features to intervene on, performs the\n",
    "        intervention, and generates a continuation, along with the logits and activations at\n",
    "        each generation position.\n",
    "        This function accepts all kwargs valid for HookedTransformer.generate(). Note that\n",
    "        freeze_attention applies only to the first token generated.\n",
    "\n",
    "        This function accepts all kwargs valid for HookedTransformer.generate(). Note that\n",
    "        direct_effects and freeze_attention apply only to the first token generated.\n",
    "\n",
    "        Note that if kv_cache is True (default), generation will be faster, as the model\n",
    "        will cache the KVs, and only process the one new token per step; if it is False,\n",
    "        the model will generate by doing a full forward pass across all tokens. Note that\n",
    "        due to numerical precision issues, you are only guaranteed that the logits /\n",
    "        activations of model.feature_intervention_generate(s, ...) are equivalent to\n",
    "        model.feature_intervention(s, ...) if kv_cache is False.\n",
    "\n",
    "        Args:\n",
    "            input (_type_): the input prompt to intervene on\n",
    "            interventions (list[tuple[int, Union[int, slice, torch.Tensor]], int,\n",
    "                Union[int, torch.Tensor]]): A list of interventions to perform, formatted as\n",
    "                a list of (layer, position, feature_idx, value)\n",
    "            constrained_layers: (range | None = None): whether to freeze all MLPs/transcoders /\n",
    "                attn patterns / layernorm denominators. This will only apply to the very first\n",
    "                token generated. If all layers are constrained, also freezes layernorm, computing\n",
    "                direct effects.\n",
    "            freeze_attention (bool): whether to freeze all attention patterns. Applies only to\n",
    "                the first token generated\n",
    "            apply_activation_function (bool): whether to apply the activation function when\n",
    "                recording the activations to be returned. This is useful to set to False for\n",
    "                testing purposes, as attribution predicts the change in pre-activation\n",
    "                feature values.\n",
    "            sparse (bool): whether to sparsify the activations in the returned cache. Setting\n",
    "                this to True will take up less memory, at the expense of slower interventions.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_intervention_hook_output = self._get_feature_intervention_hooks(\n",
    "            inputs,\n",
    "            interventions,\n",
    "            constrained_layers=constrained_layers,\n",
    "            freeze_attention=freeze_attention,\n",
    "            apply_activation_function=apply_activation_function,\n",
    "            sparse=sparse,\n",
    "        )\n",
    "\n",
    "        hooks, logit_cache, activation_cache = feature_intervention_hook_output\n",
    "\n",
    "        assert kwargs.get(\"use_past_kv_cache\", True), (\n",
    "            \"Generation is only possible with use_past_kv_cache=True\"\n",
    "        )\n",
    "        # Next, convert any open-ended interventions so they target position `0` (the\n",
    "        # only token present during the incremental forward passes performed by\n",
    "        # `generate`) and build the corresponding hooks.\n",
    "        open_ended_interventions = self._convert_open_ended_interventions(interventions)\n",
    "\n",
    "        # get new hooks that will target pos 0 / append logits / acts to the cache (not overwrite)\n",
    "        open_ended_hooks, open_ended_logits, open_ended_activations = (\n",
    "            self._get_feature_intervention_hooks(\n",
    "                inputs,\n",
    "                open_ended_interventions,\n",
    "                constrained_layers=None,\n",
    "                freeze_attention=False,\n",
    "                apply_activation_function=apply_activation_function,\n",
    "                sparse=sparse,\n",
    "                using_past_kv_cache=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # at the end of the model, clear original hooks and add open-ended hooks\n",
    "        def clear_and_add_hooks(tensor, hook):\n",
    "            self.reset_hooks()\n",
    "            for open_ended_name, open_ended_hook in open_ended_hooks:\n",
    "                self.add_hook(open_ended_name, open_ended_hook)\n",
    "\n",
    "        for name, hook in hooks:\n",
    "            self.add_hook(name, hook)\n",
    "\n",
    "        self.add_hook(\"unembed.hook_post\", clear_and_add_hooks)\n",
    "\n",
    "        generation: str = self.generate(inputs, **kwargs)  # type:ignore\n",
    "        self.reset_hooks()\n",
    "\n",
    "        logits = torch.cat((logit_cache[0], *open_ended_logits), dim=1)  # type:ignore\n",
    "        open_ended_activations = torch.stack(\n",
    "            [torch.cat(acts, dim=0) for acts in open_ended_activations],  # type:ignore\n",
    "            dim=0,\n",
    "        )\n",
    "        activation_cache = torch.stack(activation_cache)\n",
    "        activations = torch.cat((activation_cache, open_ended_activations), dim=1)\n",
    "        if sparse:\n",
    "            activations = activations.coalesce()\n",
    "\n",
    "        return generation, logits, activations\n",
    "\n",
    "    def __del__(self):\n",
    "        # Prevent memory leaks\n",
    "        self.reset_hooks(including_permanent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f960c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_reciever_heads = [(38, 25),\n",
    " (37, 30),\n",
    " (36, 38),\n",
    " (36, 37),\n",
    " (5, 17),\n",
    " (38, 10),\n",
    " (24, 21),\n",
    " (5, 16),\n",
    " (5, 2),\n",
    " (5, 0),\n",
    " (34, 31),\n",
    " (31, 3),\n",
    " (34, 34),\n",
    " (36, 36),\n",
    " (38, 26),\n",
    " (32, 2),\n",
    " (36, 21),\n",
    " (23, 16),\n",
    " (5, 3),\n",
    " (34, 4),\n",
    " (34, 2),\n",
    " (5, 7),\n",
    " (36, 23),\n",
    " (32, 3),\n",
    " (30, 9),\n",
    " (3, 26),\n",
    " (4, 27),\n",
    " (38, 31),\n",
    " (9, 39),\n",
    " (5, 23),\n",
    " (5, 24),\n",
    " (3, 8),\n",
    " (6, 30),\n",
    " (38, 14),\n",
    " (4, 28),\n",
    " (34, 3),\n",
    " (3, 27),\n",
    " (36, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff35382",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "transcoder_name = \"mwhanna/qwen3-14b-transcoders-lowl0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d757422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-14B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ee13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e97f0ff17794789bb86612305d91062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af3d9876bf8466faed6bb83e7414512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_1.safetensors:  73%|#######2  | 2.44G/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb98a99e21194e2a9d0cc404455b8b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_15.safetensors:   7%|6         | 220M/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d7e1828d594667aed7c1e5a8ea9f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_13.safetensors:   4%|4         | 147M/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda679818b034ad9979922416654d5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_10.safetensors:   6%|5         | 189M/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde93e059c3b493c9dc40dd4e1534304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_11.safetensors:   6%|6         | 210M/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a643cf45a9746c68e83545bb4101bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_0.safetensors:   4%|4         | 147M/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ca1227dde947ef8ff0b3f6e0b24104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_12.safetensors:  86%|########5 | 2.87G/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5c2c8f5ab2449dbc34d80c451f098f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_14.safetensors:   5%|5         | 178M/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af16ef7c2bbc41e1a3f81ca59c238b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_16.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3210f871b840e0aebe592164003c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_17.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f317ee9f5441818c4b65b8bc4085f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_18.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6f5d3061dc4efd9055524e0ad8b3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_19.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9594451018604087a68d33b97e7f23be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_2.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ablate_attention_heads(qwen_reciever_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_generation_sentences = [\"Wait, maybe I should try to figure out the exact path.\", \"Wait, let me try to figure out how many straight segments there are.\", \"Let me check again.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc48532",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = plan_generation_sentences[0] # What you want to get the graph for\n",
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=8  # Batch size when attributing\n",
    "offload='disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc199fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e090a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs_plan_generation_1_ablation'\n",
    "graph_name = 'plan_generation_1_ablation.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a67d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"plan_generation_1_trimmed_ablation\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graph_files_1_ablation'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8421a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "\n",
    "port = 8053\n",
    "server = serve(data_dir='./Trimmed Graphs/reciever_heads_qwen3/graph_files_1_reciever_ablation', port=port) # change data_dir to attributio0n graph\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import output as colab_output  # noqa\n",
    "    colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)\n",
    "else:\n",
    "    from IPython.display import IFrame\n",
    "    print(f\"Use the IFrame below, or open your graph here: f'http://localhost:{port}/index.html'\")\n",
    "    display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
