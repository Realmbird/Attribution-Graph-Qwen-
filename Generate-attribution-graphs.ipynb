{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1154b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/chriskino/Attribution-Graph-Qwen-/circuit-tracer\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops>=0.8.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.26.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.34.1)\n",
      "Requirement already satisfied: ipykernel>=6.29.5 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (6.30.1)\n",
      "Requirement already satisfied: ipywidgets>=8.1.7 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (2.11.7)\n",
      "Requirement already satisfied: safetensors>=0.5.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: tokenizers>=0.21.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (0.21.4)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /opt/miniconda3/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (4.66.5)\n",
      "Requirement already satisfied: transformer-lens>=v2.16.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (2.16.1)\n",
      "Requirement already satisfied: transformers>=4.50.0 in /home/chriskino/.local/lib/python3.12/site-packages (from circuit-tracer==0.1.0) (4.54.1)\n",
      "Requirement already satisfied: filelock in /home/chriskino/.local/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/chriskino/.local/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/chriskino/.local/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/chriskino/.local/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/chriskino/.local/lib/python3.12/site-packages (from huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (1.1.8)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (9.4.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (27.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/chriskino/.local/lib/python3.12/site-packages (from ipykernel>=6.29.5->circuit-tracer==0.1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/chriskino/.local/lib/python3.12/site-packages (from ipywidgets>=8.1.7->circuit-tracer==0.1.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/chriskino/.local/lib/python3.12/site-packages (from ipywidgets>=8.1.7->circuit-tracer==0.1.0) (3.0.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/chriskino/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->circuit-tracer==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/chriskino/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->circuit-tracer==0.1.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/chriskino/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->circuit-tracer==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/chriskino/.local/lib/python3.12/site-packages (from torch>=2.0.0->circuit-tracer==0.1.0) (3.4.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.3.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2.3.1)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: sentencepiece in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.2.1)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.0.5)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.4.4)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /home/chriskino/.local/lib/python3.12/site-packages (from transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.21.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/chriskino/.local/lib/python3.12/site-packages (from transformers>=4.50.0->circuit-tracer==0.1.0) (2025.7.34)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/chriskino/.local/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/chriskino/.local/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/chriskino/.local/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/chriskino/.local/lib/python3.12/site-packages (from datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: decorator in /home/chriskino/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/chriskino/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/chriskino/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/chriskino/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/chriskino/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/chriskino/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/chriskino/.local/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/chriskino/.local/lib/python3.12/site-packages (from jaxtyping>=0.2.11->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/chriskino/.local/lib/python3.12/site-packages (from jupyter-client>=8.0.0->ipykernel>=6.29.5->circuit-tracer==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/chriskino/.local/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.29.5->circuit-tracer==0.1.0) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/chriskino/.local/lib/python3.12/site-packages (from pandas>=1.1.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/chriskino/.local/lib/python3.12/site-packages (from pandas>=1.1.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.26.0->circuit-tracer==0.1.0) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/chriskino/.local/lib/python3.12/site-packages (from rich>=12.6.0->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/chriskino/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->circuit-tracer==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/chriskino/.local/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/chriskino/.local/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.1.45)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/chriskino/.local/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (6.32.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/chriskino/.local/lib/python3.12/site-packages (from wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2.35.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/chriskino/.local/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->circuit-tracer==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/chriskino/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (3.12.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/chriskino/.local/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (4.0.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/chriskino/.local/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/chriskino/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/chriskino/.local/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/chriskino/.local/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /home/chriskino/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel>=6.29.5->circuit-tracer==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/chriskino/.local/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/chriskino/.local/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/chriskino/.local/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->circuit-tracer==0.1.0) (0.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/chriskino/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/chriskino/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/chriskino/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/chriskino/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/chriskino/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/chriskino/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/chriskino/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/chriskino/.local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens>=v2.16.0->circuit-tracer==0.1.0) (5.0.2)\n",
      "Building wheels for collected packages: circuit-tracer\n",
      "  Building editable for circuit-tracer (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuit-tracer: filename=circuit_tracer-0.1.0-py3-none-any.whl size=6281 sha256=8770852c4ef29bc5e2f327300216c4e60f2ec46c0c5e8c671cc34866d47a9af3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l427jlhy/wheels/50/3e/98/6b6182be2f50cece9f65caed8fbff0135cbbdf3c8ba3867876\n",
      "Successfully built circuit-tracer\n",
      "Installing collected packages: circuit-tracer\n",
      "  Attempting uninstall: circuit-tracer\n",
      "    Found existing installation: circuit-tracer 0.1.0\n",
      "    Uninstalling circuit-tracer-0.1.0:\n",
      "      Successfully uninstalled circuit-tracer-0.1.0\n",
      "\u001b[33m  WARNING: The script circuit-tracer is installed in '/home/chriskino/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed circuit-tracer-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e circuit-tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733bebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this in a cell before importing torch/transformers\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf6664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab Setup Environment\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    !mkdir -p repository && cd repository && \\\n",
    "     git clone https://github.com/safety-research/circuit-tracer && \\\n",
    "     curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n",
    "     uv pip install -e circuit-tracer/\n",
    "\n",
    "    import sys\n",
    "    from huggingface_hub import notebook_login\n",
    "    sys.path.append('repository/circuit-tracer')\n",
    "    sys.path.append('repository/circuit-tracer/demos')\n",
    "    notebook_login(new_session=False)\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77faf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import attribute\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "from huggingface_hub import get_token, hf_api, hf_hub_download, snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff35382",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "transcoder_name = \"mwhanna/qwen3-14b-transcoders-lowl0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d998ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7494b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86dbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import parse_qs, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621e8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "class HfUri(NamedTuple):\n",
    "    \"\"\"Structured representation of a HuggingFace URI.\"\"\"\n",
    "\n",
    "    repo_id: str\n",
    "    file_path: str | None\n",
    "    revision: str | None\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, hf_ref: str):\n",
    "        if hf_ref.startswith(\"hf://\"):\n",
    "            return parse_hf_uri(hf_ref)\n",
    "\n",
    "        parts = hf_ref.split(\"@\", 1)\n",
    "        repo_id = parts[0]\n",
    "        revision = parts[1] if len(parts) > 1 else None\n",
    "        return cls(repo_id, None, revision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e0bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hf_uri(uri: str) -> HfUri:\n",
    "    \"\"\"Parse an HF URI into repo id, file path and revision.\n",
    "\n",
    "    Args:\n",
    "        uri: String like ``hf://org/repo/file?revision=main``.\n",
    "\n",
    "    Returns:\n",
    "        ``HfUri`` with repository id, file path and optional revision.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(uri)\n",
    "    if parsed.scheme != \"hf\":\n",
    "        raise ValueError(f\"Not a huggingface URI: {uri}\")\n",
    "    path = parsed.path.lstrip(\"/\")\n",
    "    repo_parts = path.split(\"/\", 1)\n",
    "    if len(repo_parts) != 2:\n",
    "        raise ValueError(f\"Invalid huggingface URI: {uri}\")\n",
    "    repo_id = f\"{parsed.netloc}/{repo_parts[0]}\"\n",
    "    file_path = repo_parts[1]\n",
    "    revision = parse_qs(parsed.query).get(\"revision\", [None])[0] or None\n",
    "    return HfUri(repo_id, file_path, revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6814186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_transcoder_paths(config: dict) -> dict:\n",
    "    if \"transcoders\" in config:\n",
    "        hf_paths = [path for path in config[\"transcoders\"] if path.startswith(\"hf://\")]\n",
    "        local_map = download_hf_uris(hf_paths)\n",
    "        transcoder_paths = {\n",
    "            i: local_map.get(path, path) for i, path in enumerate(config[\"transcoders\"])\n",
    "        }\n",
    "    else:\n",
    "        local_path = snapshot_download(\n",
    "            config[\"repo_id\"],\n",
    "            revision=config.get(\"revision\", \"main\"),\n",
    "            allow_patterns=[\"layer_*.safetensors\"],\n",
    "        )\n",
    "        layer_files = glob.glob(os.path.join(local_path, \"layer_*.safetensors\"))\n",
    "        transcoder_paths = {\n",
    "            i: os.path.join(local_path, f\"layer_{i}.safetensors\") for i in range(len(layer_files))\n",
    "        }\n",
    "    return transcoder_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ea5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections.abc import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import safe_open\n",
    "from torch import nn\n",
    "\n",
    "from circuit_tracer.transcoder.activation_functions import JumpReLU\n",
    "from circuit_tracer.utils import get_default_device\n",
    "\n",
    "\n",
    "class SingleLayerTranscoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A per-layer transcoder (PLT) that replaces MLP computation with interpretable features.\n",
    "\n",
    "    Per-layer transcoders decompose the output of a single MLP layer into sparsely active\n",
    "    features that often correspond to interpretable concepts. Unlike cross-layer transcoders,\n",
    "    each PLT operates independently on its assigned layer, which can result in longer paths\n",
    "    through attribution graphs when features amplify across multiple layers.\n",
    "\n",
    "    Attributes:\n",
    "        d_model: Dimension of the transformer's residual stream\n",
    "        d_transcoder: Number of learned features (typically >> d_model for superposition)\n",
    "        layer_idx: Which transformer layer this transcoder replaces\n",
    "        W_enc: Encoder weights mapping residual stream to feature space\n",
    "        W_dec: Decoder weights mapping features back to residual stream\n",
    "        b_enc: Encoder bias terms\n",
    "        b_dec: Decoder bias terms (reconstruction baseline)\n",
    "        W_skip: Optional skip connection weights (https://arxiv.org/abs/2501.18823)\n",
    "        activation_function: Sparsity-inducing nonlinearity (e.g., ReLU, JumpReLU)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_transcoder: int,\n",
    "        activation_function,\n",
    "        layer_idx: int,\n",
    "        skip_connection: bool = False,\n",
    "        transcoder_path: str | None = None,\n",
    "        lazy_encoder: bool = False,\n",
    "        lazy_decoder: bool = False,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if device is None:\n",
    "            device = get_default_device()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_transcoder = d_transcoder\n",
    "        self.layer_idx = layer_idx\n",
    "        self.transcoder_path = transcoder_path\n",
    "        self.lazy_encoder = lazy_encoder\n",
    "        self.lazy_decoder = lazy_decoder\n",
    "\n",
    "        if lazy_encoder or lazy_decoder:\n",
    "            assert self.transcoder_path is not None, \"Transcoder path must be set for lazy loading\"\n",
    "\n",
    "        if not lazy_encoder:\n",
    "            self.W_enc = nn.Parameter(\n",
    "                torch.zeros(d_transcoder, d_model, device=device, dtype=dtype)\n",
    "            )\n",
    "\n",
    "        if not lazy_decoder:\n",
    "            self.W_dec = nn.Parameter(\n",
    "                torch.zeros(d_transcoder, d_model, device=device, dtype=dtype)\n",
    "            )\n",
    "\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_transcoder, device=device, dtype=dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_model, device=device, dtype=dtype))\n",
    "\n",
    "        if skip_connection:\n",
    "            self.W_skip = nn.Parameter(torch.zeros(d_model, d_model, device=device, dtype=dtype))\n",
    "        else:\n",
    "            self.W_skip = None\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"Get the device of the module's parameters.\"\"\"\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        \"\"\"Get the dtype of the module's parameters.\"\"\"\n",
    "        return self.b_enc.dtype\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"Dynamically load weights when accessed if lazy loading is enabled.\"\"\"\n",
    "\n",
    "        if name == \"W_enc\" and self.lazy_encoder and self.transcoder_path is not None:\n",
    "            with safe_open(self.transcoder_path, framework=\"pt\", device=self.device.type) as f:\n",
    "                return f.get_tensor(\"W_enc\").to(self.dtype)\n",
    "        elif name == \"W_dec\" and self.lazy_decoder and self.transcoder_path is not None:\n",
    "            with safe_open(self.transcoder_path, framework=\"pt\", device=self.device.type) as f:\n",
    "                return f.get_tensor(\"W_dec\").to(self.dtype)\n",
    "\n",
    "        return super().__getattr__(name)\n",
    "\n",
    "    def _get_decoder_vectors(self, feat_ids=None):\n",
    "        to_read = feat_ids if feat_ids is not None else np.s_[:]\n",
    "        if not self.lazy_decoder:\n",
    "            return self.W_dec[to_read].to(self.dtype)\n",
    "\n",
    "        if isinstance(to_read, torch.Tensor):\n",
    "            to_read = to_read.cpu()\n",
    "        with safe_open(self.transcoder_path, framework=\"pt\", device=self.device.type) as f:\n",
    "            return f.get_slice(\"W_dec\")[to_read].to(self.dtype)\n",
    "\n",
    "    def encode(self, input_acts, apply_activation_function: bool = True):\n",
    "        W_enc = self.W_enc\n",
    "        pre_acts = F.linear(input_acts.to(W_enc.dtype), W_enc, self.b_enc)\n",
    "        if not apply_activation_function:\n",
    "            return pre_acts\n",
    "        return self.activation_function(pre_acts)\n",
    "\n",
    "    def decode(self, acts):\n",
    "        W_dec = self.W_dec\n",
    "        return acts @ W_dec + self.b_dec\n",
    "\n",
    "    def compute_skip(self, input_acts):\n",
    "        if self.W_skip is not None:\n",
    "            return input_acts @ self.W_skip.T\n",
    "        else:\n",
    "            raise ValueError(\"Transcoder has no skip connection\")\n",
    "\n",
    "    def forward(self, input_acts):\n",
    "        transcoder_acts = self.encode(input_acts)\n",
    "        decoded = self.decode(transcoder_acts)\n",
    "        decoded = decoded.detach()\n",
    "        decoded.requires_grad = True\n",
    "\n",
    "        if self.W_skip is not None:\n",
    "            skip = self.compute_skip(input_acts)\n",
    "            decoded = decoded + skip\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def encode_sparse(self, input_acts, zero_first_pos: bool = True):\n",
    "        \"\"\"Encode and return sparse activations with active encoder vectors.\n",
    "\n",
    "        Args:\n",
    "            input_acts: Input activations\n",
    "            zero_first_pos: Whether to zero out position 0\n",
    "\n",
    "        Returns:\n",
    "            sparse_acts: Sparse tensor of activations\n",
    "            active_encoders: Encoder vectors for active features only\n",
    "        \"\"\"\n",
    "        W_enc = self.W_enc\n",
    "        pre_acts = F.linear(input_acts.to(W_enc.dtype), W_enc, self.b_enc)\n",
    "        acts = self.activation_function(pre_acts)\n",
    "\n",
    "        if zero_first_pos:\n",
    "            acts[0] = 0\n",
    "\n",
    "        sparse_acts = acts.to_sparse()\n",
    "        _, feat_idx = sparse_acts.indices()\n",
    "        active_encoders = W_enc[feat_idx]\n",
    "\n",
    "        return sparse_acts, active_encoders\n",
    "\n",
    "    def decode_sparse(self, sparse_acts):\n",
    "        \"\"\"Decode sparse activations and return reconstruction with scaled decoder vectors.\n",
    "\n",
    "        Returns:\n",
    "            reconstruction: Decoded output\n",
    "            scaled_decoders: Decoder vectors scaled by activation values\n",
    "        \"\"\"\n",
    "        pos_idx, feat_idx = sparse_acts.indices()\n",
    "        values = sparse_acts.values()\n",
    "\n",
    "        # Get decoder vectors for active features only\n",
    "        W_dec = self._get_decoder_vectors(feat_idx.cpu())\n",
    "        scaled_decoders = W_dec * values[:, None]\n",
    "\n",
    "        # Reconstruct using index_add\n",
    "        n_pos = sparse_acts.shape[0]\n",
    "        reconstruction = torch.zeros(\n",
    "            n_pos, self.d_model, device=sparse_acts.device, dtype=sparse_acts.dtype\n",
    "        )\n",
    "        reconstruction = reconstruction.index_add_(0, pos_idx, scaled_decoders)\n",
    "        reconstruction = reconstruction + self.b_dec\n",
    "\n",
    "        return reconstruction, scaled_decoders\n",
    "\n",
    "\n",
    "class TranscoderSet(nn.Module):\n",
    "    \"\"\"\n",
    "    A collection of per-layer transcoders that enable construction of a replacement model.\n",
    "\n",
    "    TranscoderSet manages the collection of SingleLayerTranscoders needed for this substitution,\n",
    "    where each transcoder replaces the MLP computation at its corresponding layer.\n",
    "\n",
    "    Attributes:\n",
    "        transcoders: ModuleList of SingleLayerTranscoder instances, one per layer\n",
    "        n_layers: Total number of layers covered\n",
    "        d_transcoder: Common feature dimension across all transcoders\n",
    "        feature_input_hook: Hook point where features read from (e.g., \"hook_resid_mid\")\n",
    "        feature_output_hook: Hook point where features write to (e.g., \"hook_mlp_out\")\n",
    "        scan: Optional identifier to identify corresponding feature visualization\n",
    "        skip_connection: Whether transcoders include learned skip connections\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transcoders: dict[int, SingleLayerTranscoder],\n",
    "        feature_input_hook: str,\n",
    "        feature_output_hook: str,\n",
    "        scan: str | list[str] | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Validate that we have continuous layers from 0 to max\n",
    "        assert set(transcoders.keys()) == set(range(max(transcoders.keys()) + 1)), (\n",
    "            f\"Each layer should have a transcoder, but got transcoders for layers \"\n",
    "            f\"{set(transcoders.keys())}\"\n",
    "        )\n",
    "\n",
    "        self.transcoders = nn.ModuleList([transcoders[i] for i in range(len(transcoders))])\n",
    "        self.n_layers = len(self.transcoders)\n",
    "        self.d_transcoder = self.transcoders[0].d_transcoder\n",
    "\n",
    "        # Verify all transcoders have the same d_transcoder\n",
    "        for transcoder in self.transcoders:\n",
    "            assert transcoder.d_transcoder == self.d_transcoder, (\n",
    "                f\"All transcoders must have the same d_transcoder, but got \"\n",
    "                f\"{transcoder.d_transcoder} != {self.d_transcoder}\"\n",
    "            )\n",
    "\n",
    "        # Store hook configuration\n",
    "        self.feature_input_hook = feature_input_hook\n",
    "        self.feature_output_hook = feature_output_hook\n",
    "        self.scan = scan\n",
    "        self.skip_connection = self.transcoders[0].W_skip is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_layers\n",
    "\n",
    "    def __getitem__(self, idx: int) -> SingleLayerTranscoder:\n",
    "        return self.transcoders[idx]  # type: ignore\n",
    "\n",
    "    def __iter__(self) -> Iterator[SingleLayerTranscoder]:\n",
    "        return iter(self.transcoders)  # type: ignore\n",
    "\n",
    "    def apply_activation_function(self, layer_id, features):\n",
    "        return self.transcoders[layer_id].activation_function(features)  # type: ignore\n",
    "\n",
    "    def encode(self, input_acts):\n",
    "        return torch.stack(\n",
    "            [transcoder.encode(input_acts[i]) for i, transcoder in enumerate(self.transcoders)],  # type: ignore\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "    def _get_decoder_vectors(self, layer_id, features):\n",
    "        return self.transcoders[layer_id]._get_decoder_vectors(features)  # type: ignore\n",
    "\n",
    "    def select_decoder_vectors(self, features):\n",
    "        if not features.is_sparse:\n",
    "            features = features.to_sparse()\n",
    "\n",
    "        all_layer_idx, all_pos_idx, all_feat_idx = features.indices()\n",
    "        all_activations = features.values()\n",
    "        all_scaled_decoder_vectors = []\n",
    "        for unique_layer in all_layer_idx.unique():\n",
    "            layer_mask = all_layer_idx == unique_layer\n",
    "            feat_idx = all_feat_idx[layer_mask]\n",
    "            activations = all_activations[layer_mask]\n",
    "\n",
    "            decoder_vectors = self._get_decoder_vectors(unique_layer.item(), feat_idx)\n",
    "\n",
    "            # Multiply each activation by its corresponding decoder vector\n",
    "            scaled_decoder_vectors = activations.unsqueeze(-1) * decoder_vectors\n",
    "            all_scaled_decoder_vectors.append(scaled_decoder_vectors)\n",
    "\n",
    "        all_scaled_decoder_vectors = torch.cat(all_scaled_decoder_vectors)\n",
    "        encoder_mapping = torch.arange(features._nnz(), device=features.device)\n",
    "\n",
    "        return all_pos_idx, all_layer_idx, all_feat_idx, all_scaled_decoder_vectors, encoder_mapping\n",
    "\n",
    "    def decode(self, acts):\n",
    "        return torch.stack(\n",
    "            [transcoder.decode(acts[i]) for i, transcoder in enumerate(self.transcoders)],  # type: ignore\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "    def compute_attribution_components(\n",
    "        self,\n",
    "        mlp_inputs: torch.Tensor,\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Extract active features and their encoder/decoder vectors for attribution.\n",
    "\n",
    "        Args:\n",
    "            mlp_inputs: (n_layers, n_pos, d_model) tensor of MLP inputs\n",
    "\n",
    "        Returns:\n",
    "            Dict containing all components needed for AttributionContext:\n",
    "                - activation_matrix: Sparse (n_layers, n_pos, d_transcoder) activations\n",
    "                - reconstruction: (n_layers, n_pos, d_model) reconstructed outputs\n",
    "                - encoder_vecs: Concatenated encoder vectors for active features\n",
    "                - decoder_vecs: Concatenated decoder vectors (scaled by activations)\n",
    "                - encoder_to_decoder_map: Mapping from encoder to decoder indices\n",
    "        \"\"\"\n",
    "        device = mlp_inputs.device\n",
    "\n",
    "        reconstruction = torch.zeros_like(mlp_inputs)\n",
    "        encoder_vectors = []\n",
    "        decoder_vectors = []\n",
    "        sparse_acts_list = []\n",
    "\n",
    "        for layer, transcoder in enumerate(self.transcoders):\n",
    "            sparse_acts, active_encoders = transcoder.encode_sparse(  # type: ignore\n",
    "                mlp_inputs[layer], zero_first_pos=True\n",
    "            )\n",
    "            reconstruction[layer], active_decoders = transcoder.decode_sparse(sparse_acts)  # type: ignore\n",
    "            encoder_vectors.append(active_encoders)\n",
    "            decoder_vectors.append(active_decoders)\n",
    "            sparse_acts_list.append(sparse_acts)\n",
    "\n",
    "        activation_matrix = torch.stack(sparse_acts_list).coalesce()\n",
    "        encoder_to_decoder_map = torch.arange(activation_matrix._nnz(), device=device)\n",
    "\n",
    "        return {\n",
    "            \"activation_matrix\": activation_matrix,\n",
    "            \"reconstruction\": reconstruction,\n",
    "            \"encoder_vecs\": torch.cat(encoder_vectors, dim=0),\n",
    "            \"decoder_vecs\": torch.cat(decoder_vectors, dim=0),\n",
    "            \"encoder_to_decoder_map\": encoder_to_decoder_map,\n",
    "            \"decoder_locations\": activation_matrix.indices()[:2],\n",
    "        }\n",
    "\n",
    "    def encode_layer(self, x, layer_id, apply_activation_function=True):\n",
    "        return self.transcoders[layer_id].encode(\n",
    "            x, apply_activation_function=apply_activation_function\n",
    "        )  # type: ignore\n",
    "\n",
    "\n",
    "def load_gemma_scope_transcoder(\n",
    "    path: str,\n",
    "    layer: int,\n",
    "    device: torch.device | None = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    revision: str | None = None,\n",
    "    **kwargs,\n",
    ") -> SingleLayerTranscoder:\n",
    "    if device is None:\n",
    "        device = get_default_device()\n",
    "    if os.path.isfile(path):\n",
    "        path_to_params = path\n",
    "    else:\n",
    "        path_to_params = hf_hub_download(\n",
    "            repo_id=\"google/gemma-scope-2b-pt-transcoders\",\n",
    "            filename=path,\n",
    "            revision=revision,\n",
    "            force_download=False,\n",
    "        )\n",
    "\n",
    "    # load the parameters, have to rename the threshold key,\n",
    "    # as ours is nested inside the activation_function module\n",
    "    param_dict = np.load(path_to_params)\n",
    "    param_dict = {k: torch.tensor(v, device=device, dtype=dtype) for k, v in param_dict.items()}\n",
    "    param_dict[\"activation_function.threshold\"] = param_dict[\"threshold\"]\n",
    "    param_dict[\"W_enc\"] = param_dict[\"W_enc\"].T.contiguous()\n",
    "    del param_dict[\"threshold\"]\n",
    "\n",
    "    # create the transcoders\n",
    "    # d_model = param_dict[\"W_enc\"].shape[0]\n",
    "    # d_transcoder = param_dict[\"W_enc\"].shape[1]\n",
    "    d_transcoder, d_model = param_dict[\"W_enc\"].shape\n",
    "\n",
    "    # dummy JumpReLU; will get loaded via load_state_dict\n",
    "    activation_function = JumpReLU(torch.tensor(0.0), 0.1)\n",
    "    with torch.device(\"meta\"):\n",
    "        transcoder = SingleLayerTranscoder(d_model, d_transcoder, activation_function, layer)\n",
    "    transcoder.load_state_dict(param_dict, assign=True)\n",
    "    return transcoder\n",
    "\n",
    "\n",
    "def load_relu_transcoder(\n",
    "    path: str,\n",
    "    layer: int,\n",
    "    device: torch.device | None = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    lazy_encoder: bool = True,\n",
    "    lazy_decoder: bool = True,\n",
    "):\n",
    "    if device is None:\n",
    "        device = get_default_device()\n",
    "\n",
    "    param_dict = {}\n",
    "    with safe_open(path, framework=\"pt\", device=device.type) as f:\n",
    "        for k in f.keys():\n",
    "            if lazy_encoder and k == \"W_enc\":\n",
    "                continue\n",
    "            if lazy_decoder and k == \"W_dec\":\n",
    "                continue\n",
    "            param_dict[k] = f.get_tensor(k)\n",
    "\n",
    "    d_sae = param_dict[\"b_enc\"].shape[0]\n",
    "    d_model = param_dict[\"b_dec\"].shape[0]\n",
    "\n",
    "    assert param_dict.get(\"log_thresholds\") is None\n",
    "    activation_function = F.relu\n",
    "    with torch.device(\"meta\"):\n",
    "        transcoder = SingleLayerTranscoder(\n",
    "            d_model,\n",
    "            d_sae,\n",
    "            activation_function,\n",
    "            layer,\n",
    "            skip_connection=param_dict.get(\"W_skip\") is not None,\n",
    "            transcoder_path=path,\n",
    "            lazy_encoder=lazy_encoder,\n",
    "            lazy_decoder=lazy_decoder,\n",
    "        )\n",
    "    transcoder.load_state_dict(param_dict, assign=True)\n",
    "    return transcoder.to(dtype)\n",
    "\n",
    "\n",
    "def load_transcoder_set(\n",
    "    transcoder_paths: dict,\n",
    "    scan: str,\n",
    "    feature_input_hook: str,\n",
    "    feature_output_hook: str,\n",
    "    device: torch.device | None = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    gemma_scope: bool = False,\n",
    "    lazy_encoder: bool = True,\n",
    "    lazy_decoder: bool = True,\n",
    ") -> TranscoderSet:\n",
    "    if device is None:\n",
    "        device = get_default_device()\n",
    "    \"\"\"Loads either a preset set of transcoders, or a set specified by a file.\n",
    "\n",
    "    Args:\n",
    "        transcoder_paths: Dictionary mapping layer indices to transcoder paths\n",
    "        scan: Scan identifier\n",
    "        feature_input_hook: Hook point where features read from\n",
    "        feature_output_hook: Hook point where features write to\n",
    "        device (torch.device | None, optional): Device to load to\n",
    "        dtype (torch.dtype | None, optional): Data type to use\n",
    "        gemma_scope: Whether to use gemma scope loader\n",
    "        lazy_encoder: Whether to use lazy loading for encoder weights\n",
    "        lazy_decoder: Whether to use lazy loading for decoder weights\n",
    "\n",
    "    Returns:\n",
    "        TranscoderSet: The loaded transcoder set with all configuration\n",
    "    \"\"\"\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    devices = [torch.device(f\"cuda:{i}\") for i in range(n_gpus)] if n_gpus > 0 else [torch.device(\"cpu\")]\n",
    "\n",
    "    transcoders = {}\n",
    "    load_fn = load_gemma_scope_transcoder if gemma_scope else load_relu_transcoder\n",
    "    for layer in range(len(transcoder_paths)):\n",
    "        layer_device = devices[layer % len(devices)]\n",
    "        transcoders[layer] = load_fn(\n",
    "            transcoder_paths[layer],\n",
    "            layer,\n",
    "            device=layer_device,\n",
    "            dtype=dtype,\n",
    "            lazy_encoder=lazy_encoder,\n",
    "            lazy_decoder=lazy_decoder,\n",
    "        )\n",
    "    # we don't know how many layers the model has, but we need all layers from 0 to max covered\n",
    "    assert set(transcoders.keys()) == set(range(max(transcoders.keys()) + 1)), (\n",
    "        f\"Each layer should have a transcoder, but got transcoders for layers \"\n",
    "        f\"{set(transcoders.keys())}\"\n",
    "    )\n",
    "\n",
    "    return TranscoderSet(\n",
    "        transcoders,\n",
    "        feature_input_hook=feature_input_hook,\n",
    "        feature_output_hook=feature_output_hook,\n",
    "        scan=scan,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d56b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcoders(\n",
    "    config: dict,\n",
    "    device: torch.device | None = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    lazy_encoder: bool = False,\n",
    "    lazy_decoder: bool = True,\n",
    "):\n",
    "    \"\"\"Load a transcoder from a HuggingFace URI.\"\"\"\n",
    "\n",
    "    model_kind = config[\"model_kind\"]\n",
    "    if model_kind == \"transcoder_set\":\n",
    "        # from circuit_tracer.transcoder.single_layer_transcoder import load_transcoder_set\n",
    "\n",
    "        transcoder_paths = resolve_transcoder_paths(config)\n",
    "        is_gemma_scope = \"gemma-scope\" in config.get(\"repo_id\", \"\")\n",
    "\n",
    "        return load_transcoder_set(\n",
    "            transcoder_paths,\n",
    "            scan=config[\"scan\"],\n",
    "            feature_input_hook=config[\"feature_input_hook\"],\n",
    "            feature_output_hook=config[\"feature_output_hook\"],\n",
    "            gemma_scope=is_gemma_scope,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "            lazy_encoder=lazy_encoder,\n",
    "            lazy_decoder=lazy_decoder,\n",
    "        )\n",
    "    elif model_kind == \"cross_layer_transcoder\":\n",
    "        from circuit_tracer.transcoder.cross_layer_transcoder import load_clt\n",
    "\n",
    "        local_path = snapshot_download(\n",
    "            config[\"repo_id\"],\n",
    "            revision=config.get(\"revision\", \"main\"),\n",
    "            allow_patterns=[\"*.safetensors\"],\n",
    "        )\n",
    "\n",
    "        return load_clt(\n",
    "            local_path,\n",
    "            scan=config[\"scan\"],\n",
    "            feature_input_hook=config[\"feature_input_hook\"],\n",
    "            feature_output_hook=config[\"feature_output_hook\"],\n",
    "            lazy_decoder=lazy_decoder,\n",
    "            lazy_encoder=lazy_encoder,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model kind: {model_kind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a431d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_transcoder_from_hub(\n",
    "    hf_ref: str,\n",
    "    device: torch.device | None = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    lazy_encoder: bool = False,\n",
    "    lazy_decoder: bool = True,\n",
    "):\n",
    "    \"\"\"Load a transcoder from a HuggingFace URI.\"\"\"\n",
    "\n",
    "    # resolve legacy references\n",
    "    if hf_ref == \"gemma\":\n",
    "        hf_ref = \"mntss/gemma-scope-transcoders\"\n",
    "    elif hf_ref == \"llama\":\n",
    "        hf_ref = \"mntss/transcoder-Llama-3.2-1B\"\n",
    "\n",
    "    hf_uri = HfUri.from_str(hf_ref)\n",
    "    try:\n",
    "        config_path = hf_hub_download(\n",
    "            repo_id=hf_uri.repo_id,\n",
    "            revision=hf_uri.revision,\n",
    "            filename=\"config.yaml\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Could not download config.yaml from {hf_uri.repo_id}\") from e\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    config[\"repo_id\"] = hf_uri.repo_id\n",
    "    config[\"revision\"] = hf_uri.revision\n",
    "    config[\"scan\"] = f\"{hf_uri.repo_id}@{hf_uri.revision}\" if hf_uri.revision else hf_uri.repo_id\n",
    "\n",
    "    return load_transcoders(config, device, dtype, lazy_encoder, lazy_decoder), config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8887515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa05295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections.abc import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import safe_open\n",
    "from torch import nn\n",
    "\n",
    "from circuit_tracer.transcoder.activation_functions import JumpReLU\n",
    "from circuit_tracer.utils import get_default_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "575b0a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e36c690bdc409fb1eb528f7c79efa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transcoders, config = load_transcoder_from_hub(\n",
    "        \"mwhanna/qwen3-14b-transcoders-lowl0\",\n",
    "        dtype= torch.bfloat16,\n",
    "        lazy_encoder=True,\n",
    "        lazy_decoder=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d6700e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranscoderSet(\n",
       "  (transcoders): ModuleList(\n",
       "    (0-39): 40 x SingleLayerTranscoder()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b13796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from collections.abc import Callable\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "from circuit_tracer.attribution.context import AttributionContext\n",
    "# from circuit_tracer.transcoder import TranscoderSet\n",
    "from circuit_tracer.transcoder.cross_layer_transcoder import CrossLayerTranscoder\n",
    "from circuit_tracer.utils import get_default_device\n",
    "from circuit_tracer.utils.hf_utils import load_transcoder_from_hub\n",
    "\n",
    "\n",
    "class ReplacementModel(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-based replacement model that uses PyTorch forward hooks (no TransformerLens).\n",
    "\n",
    "    It attaches:\n",
    "      - pre_forward hooks on each layer's MLP module to capture MLP inputs\n",
    "      - forward hooks on each layer's MLP module to capture MLP outputs\n",
    "    \"\"\"\n",
    "\n",
    "    transcoders: TranscoderSet | CrossLayerTranscoder\n",
    "    feature_input_hook: str\n",
    "    feature_output_hook: str\n",
    "    skip_transcoder: bool\n",
    "    scan: str | list[str] | None\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        transcoders: TranscoderSet | CrossLayerTranscoder,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device_map: str | dict | None = None,\n",
    "        offload_folder: str | None = None,\n",
    "        use_low_cpu_mem_usage: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if device is None:\n",
    "            device = get_default_device()\n",
    "\n",
    "        load_kwargs: dict[str, Any] = {\n",
    "            \"torch_dtype\": dtype,\n",
    "            \"device_map\": device_map,\n",
    "            \"offload_folder\": offload_folder,\n",
    "            \"low_cpu_mem_usage\": use_low_cpu_mem_usage,\n",
    "        }\n",
    "        # Drop Nones\n",
    "        load_kwargs = {k: v for k, v in load_kwargs.items() if v is not None}\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None and self.tokenizer.eos_token is not None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.device_hint = device\n",
    "        self.device_map = device_map\n",
    "        self.dtype = dtype\n",
    "        self.config = self.model.config\n",
    "\n",
    "        # Attach transcoders to a primary device\n",
    "        self.primary_device = self._get_primary_device()\n",
    "        self.transcoders = transcoders.to(self.primary_device, dtype)\n",
    "\n",
    "        # Read hook configuration from transcoders and normalize to HF module + hook types\n",
    "        self.feature_input_hook = transcoders.feature_input_hook\n",
    "        self.feature_output_hook = transcoders.feature_output_hook\n",
    "        self.input_module_name, self.input_hook_type = self._normalize_hook(self.feature_input_hook)\n",
    "        self.output_module_name, self.output_hook_type = self._normalize_hook(self.feature_output_hook)\n",
    "\n",
    "        self.skip_transcoder = getattr(transcoders, \"skip_connection\", False)\n",
    "        self.scan = getattr(transcoders, \"scan\", None)\n",
    "\n",
    "        # Freeze base model; keep embeddings trainable if needed\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        if hasattr(self.model, \"get_input_embeddings\"):\n",
    "            emb = self.model.get_input_embeddings()\n",
    "            for p in emb.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        # Hook registry\n",
    "        self._hooks: dict[str, Any] = {}\n",
    "\n",
    "    # ---- Public constructors (keep API parity) ----\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained_and_transcoders(\n",
    "        cls,\n",
    "        model_name: str,\n",
    "        transcoders: TranscoderSet | CrossLayerTranscoder,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device_map: str | dict | None = None,\n",
    "        offload_folder: str | None = None,\n",
    "        use_low_cpu_mem_usage: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"ReplacementModel\":\n",
    "        return cls(\n",
    "            model_name=model_name,\n",
    "            transcoders=transcoders,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            device_map=device_map,\n",
    "            offload_folder=offload_folder,\n",
    "            use_low_cpu_mem_usage=use_low_cpu_mem_usage,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name: str,\n",
    "        transcoder_set: str,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device_map: str | dict | None = None,\n",
    "        offload_folder: str | None = None,\n",
    "        use_low_cpu_mem_usage: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"ReplacementModel\":\n",
    "        if device is None:\n",
    "            device = get_default_device()\n",
    "        transcoders, _ = load_transcoder_from_hub(transcoder_set, device=device, dtype=dtype)\n",
    "        return cls.from_pretrained_and_transcoders(\n",
    "            model_name=model_name,\n",
    "            transcoders=transcoders,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            device_map=device_map,\n",
    "            offload_folder=offload_folder,\n",
    "            use_low_cpu_mem_usage=use_low_cpu_mem_usage,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # ---- Minimal forward ----\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor | dict[str, torch.Tensor], **kwargs: Any):\n",
    "        return self.model(inputs, **kwargs) if isinstance(inputs, dict) else self.model(inputs.unsqueeze(0), **kwargs)\n",
    "\n",
    "    # ---- Utilities ----\n",
    "\n",
    "    def _get_primary_device(self) -> torch.device:\n",
    "        # Prefer HF placement if sharded; else fallback to user/device_hint/default\n",
    "        if hasattr(self.model, \"hf_device_map\") and isinstance(self.model.hf_device_map, dict):\n",
    "            first = next(iter(self.model.hf_device_map.values()))\n",
    "            return torch.device(first)\n",
    "        if self.device_hint is not None:\n",
    "            return self.device_hint\n",
    "        return get_default_device()\n",
    "\n",
    "    def _normalize_hook(self, name: str) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Map TransformerLens-style names to HF module name + hook type.\n",
    "        - '...mlp.hook_in'  -> ('mlp', 'pre_forward')\n",
    "        - '...mlp.hook_out' -> ('mlp', 'forward')\n",
    "        \"\"\"\n",
    "        lower = name.lower()\n",
    "        if \"mlp\" in lower and (\"hook_in\" in lower or \"hook_mlp_in\" in lower):\n",
    "            return \"mlp\", \"pre_forward\"\n",
    "        if \"mlp\" in lower and (\"hook_out\" in lower or \"hook_mlp_out\" in lower):\n",
    "            return \"mlp\", \"forward\"\n",
    "        # Default: treat as module path tail and use forward hook\n",
    "        return name.split(\".\")[-1], \"forward\"\n",
    "\n",
    "    def _get_layer_module(self, layer_idx: int, module_name: str):\n",
    "        base = getattr(self.model, \"model\", self.model)\n",
    "        # Try common container names\n",
    "        if hasattr(base, \"layers\"):\n",
    "            layer = base.layers[layer_idx]\n",
    "        elif hasattr(base, \"h\"):\n",
    "            layer = base.h[layer_idx]\n",
    "        elif hasattr(base, \"transformer\") and hasattr(base.transformer, \"h\"):\n",
    "            layer = base.transformer.h[layer_idx]\n",
    "        elif hasattr(base, \"blocks\"):\n",
    "            layer = base.blocks[layer_idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown base model structure: {type(base)}\")\n",
    "\n",
    "        if not hasattr(layer, module_name):\n",
    "            raise AttributeError(f\"Layer {layer_idx} has no submodule '{module_name}'.\")\n",
    "        return getattr(layer, module_name)\n",
    "\n",
    "    def _register_hook(self, layer_idx: int, module_name: str, hook_fn: Callable, hook_type: str = \"forward\"):\n",
    "        module = self._get_layer_module(layer_idx, module_name)\n",
    "        if hook_type == \"pre_forward\":\n",
    "            handle = module.register_forward_pre_hook(hook_fn)\n",
    "        elif hook_type == \"forward\":\n",
    "            handle = module.register_forward_hook(hook_fn)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown hook type: {hook_type}\")\n",
    "        key = f\"{layer_idx}.{module_name}.{hook_type}\"\n",
    "        if key in self._hooks:\n",
    "            self._hooks[key].remove()\n",
    "        self._hooks[key] = handle\n",
    "        return handle\n",
    "\n",
    "    @contextmanager\n",
    "    def zero_softcap(self):\n",
    "        # Keep API parity; HF models do not apply output softcap here.\n",
    "        yield\n",
    "\n",
    "    def ensure_tokenized(self, prompt: str | torch.Tensor | list[int]) -> torch.Tensor:\n",
    "        if isinstance(prompt, str):\n",
    "            tokens = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        elif isinstance(prompt, torch.Tensor):\n",
    "            tokens = prompt.squeeze()\n",
    "        elif isinstance(prompt, list):\n",
    "            tokens = torch.tensor(prompt, dtype=torch.long).squeeze()\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported prompt type: {type(prompt)}\")\n",
    "        if tokens.ndim > 1:\n",
    "            raise ValueError(f\"Tensor must be 1-D, got shape {tokens.shape}\")\n",
    "\n",
    "        # If first token is already special, keep\n",
    "        if hasattr(self.tokenizer, \"all_special_ids\") and len(self.tokenizer.all_special_ids) > 0:\n",
    "            if int(tokens[0].item()) in self.tokenizer.all_special_ids:\n",
    "                return tokens.to(self._get_primary_device())\n",
    "\n",
    "        # Prepend a special token to avoid pos-0 artifacts\n",
    "        candidate_bos_token_ids = [\n",
    "            getattr(self.tokenizer, \"bos_token_id\", None),\n",
    "            getattr(self.tokenizer, \"pad_token_id\", None),\n",
    "            getattr(self.tokenizer, \"eos_token_id\", None),\n",
    "        ]\n",
    "        if hasattr(self.tokenizer, \"all_special_ids\"):\n",
    "            candidate_bos_token_ids += list(self.tokenizer.all_special_ids)\n",
    "        dummy_bos_token_id = next((t for t in candidate_bos_token_ids if t is not None), None)\n",
    "        if dummy_bos_token_id is not None:\n",
    "            tokens = torch.cat([torch.tensor([dummy_bos_token_id], device=tokens.device), tokens])\n",
    "        else:\n",
    "            warnings.warn(\"No suitable special token found; the first token will be ignored.\")\n",
    "        return tokens.to(self._get_primary_device())\n",
    "\n",
    "    # ---- Optional: expose transcoder activations for debugging ----\n",
    "\n",
    "    def get_activations(\n",
    "        self,\n",
    "        inputs: str | torch.Tensor,\n",
    "        sparse: bool = False,\n",
    "        apply_activation_function: bool = True,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        target_device = self._get_primary_device()\n",
    "        if isinstance(inputs, str):\n",
    "            toks = self.tokenizer(inputs, return_tensors=\"pt\")\n",
    "            toks = {k: v.to(target_device) for k, v in toks.items()}\n",
    "        elif isinstance(inputs, torch.Tensor):\n",
    "            toks = inputs.to(target_device)\n",
    "        else:\n",
    "            toks = inputs\n",
    "\n",
    "        activation_cache: dict[int, torch.Tensor] = {}\n",
    "\n",
    "        def make_cache_pre(layer_idx: int):\n",
    "            def pre_hook(module, inps):\n",
    "                acts = inps[0]\n",
    "                dev = self.transcoders[layer_idx].device if isinstance(self.transcoders, TranscoderSet) else self.transcoders.device\n",
    "                feats = self.transcoders.encode_layer(\n",
    "                    acts if acts.device == dev else acts.to(dev),\n",
    "                    layer_idx,\n",
    "                    apply_activation_function=apply_activation_function,\n",
    "                ).detach()\n",
    "                activation_cache[layer_idx] = (feats.to_sparse() if sparse else feats).cpu()\n",
    "                return None\n",
    "            return pre_hook\n",
    "\n",
    "        def make_cache_fwd(layer_idx: int):\n",
    "            def fwd_hook(module, inps, out):\n",
    "                acts = inps[0] if len(inps) else out\n",
    "                dev = self.transcoders[layer_idx].device if isinstance(self.transcoders, TranscoderSet) else self.transcoders.device\n",
    "                feats = self.transcoders.encode_layer(\n",
    "                    acts if acts.device == dev else acts.to(dev),\n",
    "                    layer_idx,\n",
    "                    apply_activation_function=apply_activation_function,\n",
    "                ).detach()\n",
    "                activation_cache[layer_idx] = (feats.to_sparse() if sparse else feats).cpu()\n",
    "                return out\n",
    "            return fwd_hook\n",
    "\n",
    "        handles = []\n",
    "        for layer in range(self.config.num_hidden_layers):\n",
    "            if self.input_hook_type == \"pre_forward\":\n",
    "                handles.append(self._register_hook(layer, self.input_module_name, make_cache_pre(layer), \"pre_forward\"))\n",
    "            else:\n",
    "                handles.append(self._register_hook(layer, self.input_module_name, make_cache_fwd(layer), \"forward\"))\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                out = self.model(toks if isinstance(toks, dict) else toks.unsqueeze(0))\n",
    "                logits = out.logits\n",
    "            acts = torch.stack([activation_cache[i].to(logits.device) for i in range(len(activation_cache))])\n",
    "            if sparse:\n",
    "                acts = acts.coalesce()\n",
    "            return logits, acts\n",
    "        finally:\n",
    "            for h in handles:\n",
    "                h.remove()\n",
    "\n",
    "    # ---- Attribution context builder ----\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def setup_attribution(self, inputs: str | torch.Tensor) -> AttributionContext:\n",
    "        tokens_1d = self.ensure_tokenized(inputs if isinstance(inputs, str) else inputs)\n",
    "\n",
    "        mlp_in_cache: dict[int, torch.Tensor] = {}\n",
    "        mlp_out_cache: dict[int, torch.Tensor] = {}\n",
    "\n",
    "        def mk_in_pre(layer: int):\n",
    "            def pre_hook(module, inps):\n",
    "                mlp_in_cache[layer] = inps[0].detach().cpu()\n",
    "                return None\n",
    "            return pre_hook\n",
    "\n",
    "        def mk_in_fwd(layer: int):\n",
    "            def fwd_hook(module, inps, out):\n",
    "                mlp_in_cache[layer] = (inps[0] if len(inps) else out).detach().cpu()\n",
    "                return out\n",
    "            return fwd_hook\n",
    "\n",
    "        def mk_out_fwd(layer: int):\n",
    "            def fwd_hook(module, inps, out):\n",
    "                mlp_out_cache[layer] = out.detach().cpu()\n",
    "                return out\n",
    "            return fwd_hook\n",
    "\n",
    "        hooks = []\n",
    "        for layer in range(self.config.num_hidden_layers):\n",
    "            if self.input_hook_type == \"pre_forward\":\n",
    "                hooks.append(self._register_hook(layer, self.input_module_name, mk_in_pre(layer), \"pre_forward\"))\n",
    "            else:\n",
    "                hooks.append(self._register_hook(layer, self.input_module_name, mk_in_fwd(layer), \"forward\"))\n",
    "            hooks.append(self._register_hook(layer, self.output_module_name, mk_out_fwd(layer), \"forward\"))\n",
    "\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                out = self.model(tokens_1d.unsqueeze(0))\n",
    "                logits = out.logits\n",
    "\n",
    "            tgt = self.transcoders[0].device if isinstance(self.transcoders, TranscoderSet) else self.transcoders.device\n",
    "            mlp_in = torch.stack([mlp_in_cache[i].to(tgt) for i in range(len(mlp_in_cache))])\n",
    "            mlp_out = torch.stack([mlp_out_cache[i].to(tgt) for i in range(len(mlp_out_cache))])\n",
    "\n",
    "            # Squeeze batch dim (hooks capture B x S x D; we ran with B=1)\n",
    "            if mlp_in.ndim == 4 and mlp_in.shape[1] == 1:\n",
    "                mlp_in = mlp_in[:, 0]\n",
    "            if mlp_out.ndim == 4 and mlp_out.shape[1] == 1:\n",
    "                mlp_out = mlp_out[:, 0]\n",
    "\n",
    "            attribution = self.transcoders.compute_attribution_components(mlp_in)\n",
    "\n",
    "            attribution = self.transcoders.compute_attribution_components(mlp_in)\n",
    "            error_vectors = mlp_out - attribution[\"reconstruction\"]\n",
    "            error_vectors[:, 0] = 0  # ignore artificial BOS\n",
    "\n",
    "            # Token embeddings (robust to different model classes)\n",
    "            embed = self.model.get_input_embeddings()\n",
    "            token_vectors = embed(tokens_1d.to(embed.weight.device)).detach().to(tgt)\n",
    "\n",
    "            return AttributionContext(\n",
    "                activation_matrix=attribution[\"activation_matrix\"],\n",
    "                logits=logits,\n",
    "                error_vectors=error_vectors,\n",
    "                token_vectors=token_vectors,\n",
    "                decoder_vecs=attribution[\"decoder_vecs\"],\n",
    "                encoder_vecs=attribution[\"encoder_vecs\"],\n",
    "                encoder_to_decoder_map=attribution[\"encoder_to_decoder_map\"],\n",
    "                decoder_locations=attribution[\"decoder_locations\"],\n",
    "            )\n",
    "        finally:\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "\n",
    "\n",
    "# Convenience factory used by notebooks\n",
    "def create_replacement_model(\n",
    "    model_name: str,\n",
    "    transcoders: TranscoderSet | CrossLayerTranscoder,\n",
    "    device: torch.device | None = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    device_map: str | dict | None = \"auto\",\n",
    "    offload_folder: str | None = None,\n",
    "    use_low_cpu_mem_usage: bool = True,\n",
    "    **kwargs: Any,\n",
    ") -> ReplacementModel:\n",
    "    return ReplacementModel.from_pretrained_and_transcoders(\n",
    "        model_name=model_name,\n",
    "        transcoders=transcoders,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        offload_folder=offload_folder,\n",
    "        use_low_cpu_mem_usage=use_low_cpu_mem_usage,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ee7730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa558857660d4085a639ef051eab8313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_replacement_model(\n",
    "    model_name=model_name,\n",
    "    transcoders=transcoders,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_generation_sentences = [\"Wait, maybe I should try to figure out the exact path.\", \"Wait, let me try to figure out how many straight segments there are.\", \"Let me check again.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = plan_generation_sentences[0] # What you want to get the graph for\n",
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=8  # Batch size when attributing\n",
    "offload='disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e61fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.21 GiB. GPU 0 has a total capacity of 23.58 GiB of which 5.63 GiB is free. Process 3746032 has 280.00 MiB memory in use. Including non-PyTorch memory, this process has 17.66 GiB memory in use. Of the allocated memory 17.32 GiB is allocated by PyTorch, and 44.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m graph = \u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Attribution-Graph-Qwen-/circuit-tracer/circuit_tracer/attribution/attribute.py:137\u001b[39m, in \u001b[36mattribute\u001b[39m\u001b[34m(prompt, model, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, update_interval)\u001b[39m\n\u001b[32m    135\u001b[39m offload_handles = []\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m reload_handle \u001b[38;5;129;01min\u001b[39;00m offload_handles:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Attribution-Graph-Qwen-/circuit-tracer/circuit_tracer/attribution/attribute.py:177\u001b[39m, in \u001b[36m_run_attribution\u001b[39m\u001b[34m(model, prompt, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, offload_handles, logger, update_interval)\u001b[39m\n\u001b[32m    174\u001b[39m phase_start = time.time()\n\u001b[32m    175\u001b[39m input_ids = model.ensure_tokenized(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m ctx = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_attribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m activation_matrix = ctx.activation_matrix\n\u001b[32m    180\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecomputation completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mphase_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 357\u001b[39m, in \u001b[36mReplacementModel.setup_attribution\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mlp_out.ndim == \u001b[32m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mlp_out.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    355\u001b[39m     mlp_out = mlp_out[:, \u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m attribution = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtranscoders\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_attribution_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m attribution = \u001b[38;5;28mself\u001b[39m.transcoders.compute_attribution_components(mlp_in)\n\u001b[32m    360\u001b[39m error_vectors = mlp_out - attribution[\u001b[33m\"\u001b[39m\u001b[33mreconstruction\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 317\u001b[39m, in \u001b[36mTranscoderSet.compute_attribution_components\u001b[39m\u001b[34m(self, mlp_inputs)\u001b[39m\n\u001b[32m    314\u001b[39m sparse_acts_list = []\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer, transcoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.transcoders):\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     sparse_acts, active_encoders = \u001b[43mtranscoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmlp_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_first_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     reconstruction[layer], active_decoders = transcoder.decode_sparse(sparse_acts)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    321\u001b[39m     encoder_vectors.append(active_encoders)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mSingleLayerTranscoder.encode_sparse\u001b[39m\u001b[34m(self, input_acts, zero_first_pos)\u001b[39m\n\u001b[32m    163\u001b[39m sparse_acts = acts.to_sparse()\n\u001b[32m    164\u001b[39m _, feat_idx = sparse_acts.indices()\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m active_encoders = \u001b[43mW_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sparse_acts, active_encoders\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 6.21 GiB. GPU 0 has a total capacity of 23.58 GiB of which 5.63 GiB is free. Process 3746032 has 280.00 MiB memory in use. Including non-PyTorch memory, this process has 17.66 GiB memory in use. Of the allocated memory 17.32 GiB is allocated by PyTorch, and 44.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab573a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'plan_generation_1.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"plan_generation_1_trimmed\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
